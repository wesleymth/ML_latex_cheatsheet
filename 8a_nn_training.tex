
\section{Neural Network Training}

% \subsection*{Training of NNs with SGD}
% - SGD algorithm: Uniformly sample $n$, compute the gradient of $\mathscr{L}_{n}=\frac{1}{2}\left(y_{n}-f\left(x_{n}\right)\right)^{2}$ to update:

% $
% \left(w_{i, j}^{(l)}\right)_{t+1}=\left(w_{i, j}^{(l)}\right)_{t}-\gamma \frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}\quad\left(b_{i}^{(l)}\right)_{t+1}=\left(b_{i}^{(l)}\right)_{t}-\gamma \frac{\partial \mathscr{L}_{n}}{\partial b_{i}^{(l)}}
% $

% - In Practice: Step size schedule, mini-batch, momentum, Adam

% - Problem: With $O\left(K^{2} L\right)$ parameters, applying chain-rules independently is inefficient due to the compositional structure of $f$

% - Solution: the Backpropagation algorithm computes gradients via the chain rule but reuses intermediate computations

% \subsection*{Description of NN parameters}

$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times K}$, $\mathbf{W}^{(l)} \in \mathbb{R}^{K \times K}$ for $2 \leq l \leq L$, $\mathbf{W}^{(L+1)} \in \mathbb{R}^{K}$, $b^{(l)} \in \mathbb{R}^{K}$ for $1 \leq l \leq L$, $b^{(L+1)} \in \mathbb{R}$

\subsection*{Compact description of output}

\begin{itemize}
  % \item $x^{(1)}=f^{(1)}\left(x^{(0)}\right):=\phi\left(\left(\mathbf{W}^{(1)}\right)^{\top} x^{(0)}+b^{(1)}\right)$
  \item $x^{(l)}=f^{(l)}\left(x^{(l-1)}\right):=\phi\left(\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)}\right)$
  \item $y=f^{(L+1)}\left(x^{(L)}\right):=\left(\mathbf{W}^{(L+1)}\right)^{\top} x^{(L)}+b^{(L+1)}$
\end{itemize}

% The overall function $y=f\left(x^{(0)}\right)$ is just the composition of the layer functions:

$
f=f^{(L+1)} \circ f^{(L)} \circ \cdots \circ f^{(l)} \circ \cdots \circ f^{(2)} \circ f^{(1)}
$

% - Cost function:
$
\mathscr{L}=\frac{1}{2 N} \sum_{n=1}^{N}\left(y_{n}-f^{(L+1)} \circ \cdots \circ f^{(2)} \circ f^{(1)}\left(x_{n}\right)\right)^{2}
$

% - Remarks:

% \begin{itemize}
%   \item The specific form of the loss is not crucial
%   \item $\mathscr{L}$ is a function of all weight matrices and bias vectors
%   \item Each function $f^{(l)}$ is parameterized by weights $\mathbf{W}^{(l)}$ and biases $b^{(l)}$
% \end{itemize}

% \subsection*{Chain rule}

% $
% \mathscr{L}_{n}=\frac{1}{2}\left(y_{n}-f^{(L+1)} \circ \cdots \circ f^{(l+1)} \circ \phi(\underbrace{\left.\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)}\right)}_{z^{(l)}})\right)^{2}
% $



% $
% \frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}} \quad ?
% $

\subsection*{Chain rule}
$
\mathscr{L}_{n}=\frac{1}{2}\left(y_{n}-f^{(L+1)} \circ \cdots \circ f^{(l+1)} \circ \phi\left(z^{(l)}\right)\right)^{2}
$

% \includegraphics*[width=\columnwidth]{figures/nn4.png}

$
\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}} =\sum_{k=1}^{K} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l)}} \frac{\partial z_{k}^{(l)}}{\partial w_{i, j}^{(l)}}
=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \frac{\partial z_{j}^{(l)}}{\partial w_{i, j}^{(l)}} 
=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \cdot x_{i}^{(l-1)}
\text { since } \frac{\partial z_{k}^{(l)}}{\partial w_{i, j}^{(l)}}=0 \text { for } k \neq j,
\text { since } z_{j}^{(l)}=\sum_{k=1}^{K} w_{k, j}^{(l)} x_{k}^{(l-1)}+b_{j}^{(l)}
$

% We need to compute $\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}}, z^{(l)}, x_{i}^{(l-1)}$ and reuse them for different $\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}$

% \subsection*{Forward Pass}
% $
% x^{(0)}=x_{n} \in \mathbb{R}^{d}
% z^{(l)}=\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)} \ , \
% x^{(l)}=\phi\left(z^{(l)}\right)
% $

% Computational complexity $\Rightarrow$ $O\left(K^{2} L\right)$

% \subsection*{Backward pass (I)}
% Define $\delta_{j}^{(l)}=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}}=\sum_{k} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l+1)}} \frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}=\sum_{k} \delta_{k}^{(l+1)} \frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}
% $

% \subsection*{Backward pass (II)}
% - Using $z_{k}^{(l+1)}=\sum_{i=1}^{K} w_{i, k}^{(l+1)} x_{i}^{(l)}+b_{k}^{(l+1)}=\sum_{i=1}^{K} w_{i, k}^{(l+1)} \phi\left(z_{i}^{(l)}\right)+b_{k}^{(l+1)}$

% - We obtain $\frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}=\phi^{\prime}\left(z_{j}^{(l)}\right) w_{j, k}^{(l+1)}$

% - Thus
% $
% \delta_{j}^{(l)}=\sum_{k} \delta_{k}^{(l+1)} \phi^{\prime}\left(z_{j}^{(l)}\right) w_{j, k}^{(l+1)}
% $

% $
% \delta^{(l)}=\left(\mathbf{W}^{(l+1)} \delta^{(l+1)}\right) \odot \phi^{\prime}\left(z^{(l)}\right)
% $

% \subsection*{Backward pass (III)}
% Initialization (first derivative):

% $
%
% \delta^{(L+1)} =\frac{\partial}{\partial z^{(L+1)}} \frac{1}{2}\left(y_{n}-z^{(L+1)}\right)^{2}
% =z^{(L+1)}-y_{n}
%
% $

% $
% \delta^{(l)}=\left(\mathbf{W}^{(l+1)} \delta^{(l+1)}\right) \odot \phi^{\prime}\left(z^{(l)}\right)
% $

% Computational complexity: $O\left(K^{2} L\right)$

\subsection*{Derivatives computation}


- Using $z_{m}^{(l)}=\sum_{k=1}^{K} w_{k, m}^{(l)} x_{k}^{(l-1)}+b_{m}^{(l)}$ :

$\frac{\partial \mathscr{L}_{n}}{\partial b_{j}^{(l)}}=\sum_{k=1}^{K} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l)}} \frac{\partial z_{k}^{(l)}}{\partial b_{j}^{(l)}}=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \frac{\partial z_{j}^{(l)}}{\partial b_{j}^{(l)}}=\delta_{j}^{(l)}$

$\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}=\sum_{k=1}^{K} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l)}} \frac{\partial z_{k}^{(l)}}{\partial w_{i, j}^{(l)}}=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \frac{\partial z_{j}^{(l)}}{\partial w_{i, j}^{(l)}} =\delta_{j}^{(l)} \cdot x_{i}^{(l-1)}$

\subsection*{Backpropagation algorithm}
Forward pass $\mathcal{O}\left(K^{2} L\right)$:

$
x^{(0)}=x_{n} \in \mathbb{R}^{d},
z^{(l)}=\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)},
x^{(l)}=\phi\left(z^{(l)}\right)
$

Backward pass $\mathcal{O}\left(K^{2} L\right)$:

$
\delta^{(L+1)}=z^{(L+1)}-y_{n},
\delta^{(l)}=\left(\mathbf{W}^{(l+1)} \delta^{(l+1)}\right) \odot \phi^{\prime}\left(z^{(l)}\right)
$



Compute the derivatives:
$
\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}=\delta_{j}^{(l)} x_{i}^{(l-1)},
\frac{\partial \mathscr{L}_{n}}{\partial b_{j}^{(l)}}=\delta_{j}^{(l)}
$

% \subsection*{Parameter Initialization}
% \begin{itemize}
%   \item In deep networks, improper parameter initialization can lead to the vanishing or exploding gradients problem

%   \item Problem: Extremely slow or unstable optimization

%   \item Solution: Control the layerwise variance of neurons (aka He initialization)

% \end{itemize}

% \subsection*{Variance-Preserving Initialization}
% - for ReLU networks:

% \begin{itemize}
%   \item $z^{(l)} \sim \mathcal{N}\left(0, \mathbf{I}_{K}\right)$ : pre-activations at layer $l$ ($\operatorname{Var}\left[z_{i}^{(l)}\right]=1$ )

%   \item $w_{i}^{(l+1)} \sim \mathcal{N}\left(0, \sigma \mathbf{I}_{K}\right)$ : the $i$-th weight vector at layer $l+1$

%   \item $z_{i}^{(l+1)}=\operatorname{ReLU}\left(z^{(l)}\right)^{\top} w_{i}^{(l+1)}$
%   \item We set $\sigma=\sqrt{2 / K}$ so that $\operatorname{Var}\left[z_{i}^{(l+1)}\right]=1$

% \end{itemize}


\subsection*{Batch Normalization}
% - Batch $B=\left(x_{1}, \cdots, x_{M}\right)$ and denote by $z_{n}^{(l)}$ the layer's pre-activation input corresponding to the observation $x_{n}$

% - Step 1: Normalize each layer's input using its mean and its variance over the batch:
$
\bar{z}_{n}^{(l)}=\frac{z_{n}^{(l)}-\mu_{B}^{(l)}}{\sqrt{\left(\sigma_{B}^{(l)}\right)^{2}+\varepsilon}}
$ (Component-wise)
where $\mu_{B}^{(l)}=\frac{1}{M} \sum_{n=1}^{M} z_{n}^{(l)}$ and $\left(\sigma_{B}^{(l)}\right)^{2}=\frac{1}{M} \sum_{n=1}^{M}\left(z_{n}^{(l)}-\mu_{B}^{(l)}\right)^{2}$, and $\varepsilon \in \mathbb{R}_{\geq 0}$ is a small value added for numerical stability

- Introduce learnable parameters $\gamma^{(l)}, \beta^{(l)} \in \mathbb{R}^{K}$ to reverse the normalization :
$
\hat{z}_{n}^{(l)}=\gamma^{(l)} \odot \bar{z}_{n}^{(l)}+\beta^{(l)}
$

- Scale-invariance: 
% For $\varepsilon \approx 0$, the output is invariant to activation-wise affine scaling of $z_{n}^{(l)}$
$
\mathrm{BN}\left(a \odot z_{n}^{(l)}+b\right)=\mathrm{BN}\left(z_{n}^{(l)}\right) 
$
% $\text { for } a \in \mathbb{R}_{>0}^{K} \text { and } b \in \mathbb{R}^{K}$
% e.g. $\Rightarrow$ no need to include a bias before BatchNorm.

- Inference: Estimate $\hat{\mu}^{(l)}=\mathbf{E}\left[\mu_{B}^{(l)}\right]$ and $\hat{\sigma}^{(l)}=\mathbf{E}\left[\sigma_{B}^{(l)}\right]$ during training, use these for inference

\subsection*{Layer Normalization}
% - Step 1: Normalize each layer's input using its mean and its variance over the features (instead of over the inputs):
$
\bar{z}_{n}^{(l)}=\frac{z_{n}^{(l)}-\mu_{n}^{(l)} \cdot 1_{K}}{\sqrt{\left(\sigma_{n}^{(l)}\right)^{2}+\varepsilon}}
$
where $\mu_{n}^{(l)}=\frac{1}{K} \sum_{k=1}^{K} z_{n}^{(l)}(k)$ and $\left(\sigma_{n}^{(l)}\right)^{2}=\frac{1}{K} \sum_{k=1}^{K}\left(z_{n}^{(l)}(k)-\mu_{n}^{(l)}\right)^{2}$, and $\varepsilon \in \mathbb{R}_{\geq 0}$

- Learnable parameters $\gamma^{(l)}, \beta^{(l)} \in \mathbb{R}^{K}$ :
$
\hat{z}_{n}^{(l)}=\gamma^{(l)} \odot \bar{z}_{n}^{(l)}+\beta^{(l)}
$

\begin{itemize}
  \item Normalize across features, independently for each observation
  \item common alternative, widely used for transformers and text data
  \item No batch dependency, use the same for training and inference
\end{itemize}

% \subsection*{Normalization Benefits}

% \begin{itemize}
%   \item Stabilizes activation magnitudes / reduces initialization impact
%   \item Additional regularization effect from noisy $\mu_{B}^{(l)}, \sigma_{B}^{(l)}$ in batch norm
%   \item Stabilizes and speeds up training, allows larger learning rates
%   \item Used in almost all modern deep learning architectures
%   \item Often inserted after every convolutional layer, before non-linearity
% \end{itemize}

% \subsection*{Recap}
% \begin{itemize}
%   \item Neural networks are trained with gradient-based methods such as SGD

%   \item To compute the gradients, we use backpropagation, which involves the chain rule to efficiently calculate the gradients based on the network's intermediate outputs $z^{(l)}$ and $\delta^{(l)}$

%   \item Proper parameter initialization should avoid exploding and vanishing gradients by carefully controlling the layerwise variance

%   \item Batch and Layer normalization dynamically stabilize the training process, allowing for faster convergence and the use of larger learning rates

% \end{itemize}