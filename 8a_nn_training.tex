
\section*{Neural Network Training}

\subsection*{Training of NNs with SGD}
- SGD algorithm: Uniformly sample $n$, compute the gradient of $\mathscr{L}_{n}=\frac{1}{2}\left(y_{n}-f\left(x_{n}\right)\right)^{2}$ to update:

$
\left(w_{i, j}^{(l)}\right)_{t+1}=\left(w_{i, j}^{(l)}\right)_{t}-\gamma \frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}} \\\quad\left(b_{i}^{(l)}\right)_{t+1}=\left(b_{i}^{(l)}\right)_{t}-\gamma \frac{\partial \mathscr{L}_{n}}{\partial b_{i}^{(l)}}
$

- In Practice: Step size schedule, mini-batch, momentum, Adam

- Problem: With $O\left(K^{2} L\right)$ parameters, applying chain-rules independently is inefficient due to the compositional structure of $f$

- Solution: the Backpropagation algorithm computes gradients via the chain rule but reuses intermediate computations

\subsection*{Description of NN parameters}


Weight matrices: $\mathbf{W}^{(l)}$ such that $\mathbf{W}_{i, j}^{(l)}=w_{i, j}^{(l)}$, of size

\begin{itemize}
  \item $\mathbf{W}^{(1)} \in \mathbb{R}^{d \times K}$
  \item $\mathbf{W}^{(l)} \in \mathbb{R}^{K \times K}$ for $2 \leq l \leq L$
  \item $\mathbf{W}^{(L+1)} \in \mathbb{R}^{K}$
\end{itemize}

Bias vectors: $b^{(l)}$ such that the i-th component is $b_{i}^{(l)}$

\begin{itemize}
  \item $b^{(l)} \in \mathbb{R}^{K}$ for $1 \leq l \leq L$
  \item $b^{(L+1)} \in \mathbb{R}$
\end{itemize}

\subsection*{Compact description of output}

\begin{itemize}
  \item $x^{(1)}=f^{(1)}\left(x^{(0)}\right):=\phi\left(\left(\mathbf{W}^{(1)}\right)^{\top} x^{(0)}+b^{(1)}\right)$
  \item $x^{(l)}=f^{(l)}\left(x^{(l-1)}\right):=\phi\left(\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)}\right)$
  \item $y=f^{(L+1)}\left(x^{(L)}\right):=\left(\mathbf{W}^{(L+1)}\right)^{\top} x^{(L)}+b^{(L+1)}$
\end{itemize}

The overall function $y=f\left(x^{(0)}\right)$ is just the composition of the layer functions:

$
f=f^{(L+1)} \circ f^{(L)} \circ \cdots \circ f^{(l)} \circ \cdots \circ f^{(2)} \circ f^{(1)}
$

\subsection*{Cost function}

$
\mathscr{L}=\frac{1}{2 N} \sum_{n=1}^{N}\left(y_{n}-f^{(L+1)} \circ \cdots \circ f^{(2)} \circ f^{(1)}\left(x_{n}\right)\right)^{2}
$

% - Remarks:

% \begin{itemize}
%   \item The specific form of the loss is not crucial
%   \item $\mathscr{L}$ is a function of all weight matrices and bias vectors
%   \item Each function $f^{(l)}$ is parameterized by weights $\mathbf{W}^{(l)}$ and biases $b^{(l)}$
% \end{itemize}

% \subsection*{Chain rule}

% $
% \mathscr{L}_{n}=\frac{1}{2}\left(y_{n}-f^{(L+1)} \circ \cdots \circ f^{(l+1)} \circ \phi(\underbrace{\left.\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)}\right)}_{z^{(l)}})\right)^{2}
% $



% $
% \frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}} \quad ?
% $

\subsection*{Chain rule}
$
\mathscr{L}_{n}=\frac{1}{2}\left(y_{n}-f^{(L+1)} \circ \cdots \circ f^{(l+1)} \circ \phi\left(z^{(l)}\right)\right)^{2}
$

\includegraphics*[width=\columnwidth]{figures/nn4.png}

\scalebox{0.9}{
$
\begin{aligned}
\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}} & =\sum_{k=1}^{K} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l)}} \frac{\partial z_{k}^{(l)}}{\partial w_{i, j}^{(l)}} \\
& =\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \frac{\partial z_{j}^{(l)}}{\partial w_{i, j}^{(l)}} \quad \text { since } \frac{\partial z_{k}^{(l)}}{\partial w_{i, j}^{(l)}}=0 \text { for } k \neq j \\
& =\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \cdot x_{i}^{(l-1)} \quad \text { since } z_{j}^{(l)}=\sum_{k=1}^{K} w_{k, j}^{(l)} x_{k}^{(l-1)}+b_{j}^{(l)}
\end{aligned}
$
}

We need to compute $\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}}, z^{(l)}, x_{i}^{(l-1)}$ and reuse them for different $\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}$

\subsection*{Forward Pass}
$
x^{(0)}=x_{n} \in \mathbb{R}^{d} \\
z^{(l)}=\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)} \ , \
x^{(l)}=\phi\left(z^{(l)}\right)
$

Computational complexity $\Rightarrow$ $O\left(K^{2} L\right)$

\subsection*{Backward pass (I)}
Define $\delta_{j}^{(l)}=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}}=\sum_{k} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l+1)}} \frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}=\sum_{k} \delta_{k}^{(l+1)} \frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}
$

\subsection*{Backward pass (II)}
- Using $z_{k}^{(l+1)}=\sum_{i=1}^{K} w_{i, k}^{(l+1)} x_{i}^{(l)}+b_{k}^{(l+1)}=\sum_{i=1}^{K} w_{i, k}^{(l+1)} \phi\left(z_{i}^{(l)}\right)+b_{k}^{(l+1)}$

- We obtain $\frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}=\phi^{\prime}\left(z_{j}^{(l)}\right) w_{j, k}^{(l+1)}$

- Thus
$
\delta_{j}^{(l)}=\sum_{k} \delta_{k}^{(l+1)} \phi^{\prime}\left(z_{j}^{(l)}\right) w_{j, k}^{(l+1)}
$

$
\delta^{(l)}=\left(\mathbf{W}^{(l+1)} \delta^{(l+1)}\right) \odot \phi^{\prime}\left(z^{(l)}\right)
$

\subsection*{Backward pass (III)}
Initialization (first derivative):

$
\begin{aligned}
\delta^{(L+1)} & =\frac{\partial}{\partial z^{(L+1)}} \frac{1}{2}\left(y_{n}-z^{(L+1)}\right)^{2} \\
& =z^{(L+1)}-y_{n}
\end{aligned}
$

$
\delta^{(l)}=\left(\mathbf{W}^{(l+1)} \delta^{(l+1)}\right) \odot \phi^{\prime}\left(z^{(l)}\right)
$

Computational complexity: $O\left(K^{2} L\right)$

\subsection*{Derivatives computation}


- Using $z_{m}^{(l)}=\sum_{k=1}^{K} w_{k, m}^{(l)} x_{k}^{(l-1)}+b_{m}^{(l)}$ :

$\frac{\partial \mathscr{L}_{n}}{\partial b_{j}^{(l)}}=\sum_{k=1}^{K} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l)}} \frac{\partial z_{k}^{(l)}}{\partial b_{j}^{(l)}}=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \frac{\partial z_{j}^{(l)}}{\partial b_{j}^{(l)}}=\delta_{j}^{(l)}$

$\frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}=\sum_{k=1}^{K} \frac{\partial \mathscr{L}_{n}}{\partial z_{k}^{(l)}} \frac{\partial z_{k}^{(l)}}{\partial w_{i, j}^{(l)}}=\frac{\partial \mathscr{L}_{n}}{\partial z_{j}^{(l)}} \frac{\partial z_{j}^{(l)}}{\partial w_{i, j}^{(l)}}\\ \qquad =\delta_{j}^{(l)} \cdot x_{i}^{(l-1)}$

\subsection*{Backpropagation algorithm}
Forward pass:

$
\begin{aligned}
& x^{(0)}=x_{n} \in \mathbb{R}^{d} \\
& z^{(l)}=\left(\mathbf{W}^{(l)}\right)^{\top} x^{(l-1)}+b^{(l)} \\
& x^{(l)}=\phi\left(z^{(l)}\right)
\end{aligned}
$

Backward pass:

$
\begin{aligned}
& \delta^{(L+1)}=z^{(L+1)}-y_{n} \\
& \delta^{(l)}=\left(\mathbf{W}^{(l+1)} \delta^{(l+1)}\right) \odot \phi^{\prime}\left(z^{(l)}\right)
\end{aligned}
$



Compute the derivatives:

$
\begin{aligned}
& \frac{\partial \mathscr{L}_{n}}{\partial w_{i, j}^{(l)}}=\delta_{j}^{(l)} x_{i}^{(l-1)} \\
& \frac{\partial \mathscr{L}_{n}}{\partial b_{j}^{(l)}}=\delta_{j}^{(l)}
\end{aligned}
$

\subsection*{Parameter Initialization}
\begin{itemize}
  \item In deep networks, improper parameter initialization can lead to the vanishing or exploding gradients problem

  \item Problem: Extremely slow or unstable optimization

  \item Solution: Control the layerwise variance of neurons (aka He initialization)

\end{itemize}

\subsection*{Variance-Preserving Initialization}
- for ReLU networks:

\begin{itemize}
  \item $z^{(l)} \sim \mathcal{N}\left(0, \mathbf{I}_{K}\right)$ : pre-activations at layer $l$ (note: $\operatorname{Var}\left[z_{i}^{(l)}\right]=1$ )

  \item $w_{i}^{(l+1)} \sim \mathcal{N}\left(0, \sigma \mathbf{I}_{K}\right)$ : the $i$-th weight vector at layer $l+1$

  \item $z_{i}^{(l+1)}=\operatorname{ReLU}\left(z^{(l)}\right)^{\top} w_{i}^{(l+1)}:$ the $i$-th pre-activation at layer $l+1$
  \item We set $\sigma=\sqrt{2 / K}$ so that $\operatorname{Var}\left[z_{i}^{(l+1)}\right]=1$

\end{itemize}


\subsection*{Batch Normalization}
- Batch $B=\left(x_{1}, \cdots, x_{M}\right)$ and denote by $z_{n}^{(l)}$ the layer's pre-activation input corresponding to the observation $x_{n}$

- Step 1: Normalize each layer's input using its mean and its variance over the batch:

$
\bar{z}_{n}^{(l)}=\frac{z_{n}^{(l)}-\mu_{B}^{(l)}}{\sqrt{\left(\sigma_{B}^{(l)}\right)^{2}+\varepsilon}}
$ (Component-wise)

where $\mu_{B}^{(l)}=\frac{1}{M} \sum_{n=1}^{M} z_{n}^{(l)}$ and $\left(\sigma_{B}^{(l)}\right)^{2}=\frac{1}{M} \sum_{n=1}^{M}\left(z_{n}^{(l)}-\mu_{B}^{(l)}\right)^{2}$, and $\varepsilon \in \mathbb{R}_{\geq 0}$ is a small value added for numerical stability

- Step 2: Introduce learnable parameters $\gamma^{(l)}, \beta^{(l)} \in \mathbb{R}^{K}$ to be able to reverse the normalization if needed:
$
\hat{z}_{n}^{(l)}=\gamma^{(l)} \odot \bar{z}_{n}^{(l)}+\beta^{(l)}
$

- Scale-invariance: For $\varepsilon \approx 0$, the output is invariant to activation-wise affine scaling of $z_{n}^{(l)}$

$
\mathrm{BN}\left(a \odot z_{n}^{(l)}+b\right)=\mathrm{BN}\left(z_{n}^{(l)}\right) \text { for } a \in \mathbb{R}_{>0}^{K} \text { and } b \in \mathbb{R}^{K}
$ e.g. $\Rightarrow$ no need to include a bias before BatchNorm.

- Inference: The prediction for one sample should not depend on other samples

\begin{itemize}
  \item Estimate $\hat{\mu}^{(l)}=\mathbf{E}\left[\mu_{B}^{(l)}\right]$ and $\hat{\sigma}^{(l)}=\mathbf{E}\left[\sigma_{B}^{(l)}\right]$ during training, use these for inference
  \item Exponential moving averages are commonly used in practice
\end{itemize}

- Implementation:

\begin{itemize}
  \item Requires sufficiently large batches to get good estimates of $\mu_{B}^{(l)}, \sigma_{B}^{(l)}$
  \item BatchNorm is applied a bit differently for non-fully-connected nets
\end{itemize}

\subsection*{Batch Normalization - Results}

\begin{itemize}
  \item BatchNorm leads to much faster convergence
  \item BatchNorm allows to use much larger learning rates (up to $30 \times$ )
\end{itemize}

\subsection*{Layer Normalization}
- Step 1: Normalize each layer's input using its mean and its variance over the features (instead of over the inputs):
$
\bar{z}_{n}^{(l)}=\frac{z_{n}^{(l)}-\mu_{n}^{(l)} \cdot 1_{K}}{\sqrt{\left(\sigma_{n}^{(l)}\right)^{2}+\varepsilon}}
$

where $\mu_{n}^{(l)}=\frac{1}{K} \sum_{k=1}^{K} z_{n}^{(l)}(k)$ and $\left(\sigma_{n}^{(l)}\right)^{2}=\frac{1}{K} \sum_{k=1}^{K}\left(z_{n}^{(l)}(k)-\mu_{n}^{(l)}\right)^{2}$, and $\varepsilon \in \mathbb{R}_{\geq 0}$

- Step 2: Introduce learnable parameters $\gamma^{(l)}, \beta^{(l)} \in \mathbb{R}^{K}$ :
$
\hat{z}_{n}^{(l)}=\gamma^{(l)} \odot \bar{z}_{n}^{(l)}+\beta^{(l)}
$

\begin{itemize}
  \item Normalize across features, independently for each observation
  \item Very common alternative, widely used for transformers and text data
  \item No batch dependency, use the same for training and inference
\end{itemize}

\subsection*{Normalization Benefits}

\begin{itemize}
  \item Stabilizes activation magnitudes / reduces initialization impact
  \item Additional regularization effect from noisy $\mu_{B}^{(l)}, \sigma_{B}^{(l)}$ in batch norm
  \item Stabilizes and speeds up training, allows larger learning rates
  \item Used in almost all modern deep learning architectures
  \item Often inserted after every convolutional layer, before non-linearity
\end{itemize}

\subsection*{Recap}
\begin{itemize}
  \item Neural networks are trained with gradient-based methods such as SGD

  \item To compute the gradients, we use backpropagation, which involves the chain rule to efficiently calculate the gradients based on the network's intermediate outputs $z^{(l)}$ and $\delta^{(l)}$

  \item Proper parameter initialization should avoid exploding and vanishing gradients by carefully controlling the layerwise variance

  \item Batch and Layer normalization dynamically stabilize the training process, allowing for faster convergence and the use of larger learning rates

\end{itemize}