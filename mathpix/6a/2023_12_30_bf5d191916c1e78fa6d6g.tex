\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{mathrsfs}
\usepackage{bbold}

\title{Support Vector Machines }


\author{Machine Learning Course - CS-433\\
Oct 24, 2023\\
Nicolas Flammarion}
\date{}


\begin{document}
\maketitle
EPFL

\section*{Vapnik's invention}
\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-02}
\end{center}

\section*{Binary classification}
We observe some data $S=\left\{x_{n}, y_{n}\right\}_{n=1}^{N} \in \mathscr{X} \times\{-1,1\}$

Goal: given a new observation $x$, we want to predict its label $y$

How:

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-03}
\end{center}

\section*{Linear Classifier}
Define a hyperplane as $\left\{x: w^{\top} x=0\right\}$ where $\|w\|=1$

Prediction:

$$
f(x)=\operatorname{sign}\left(x^{\top} w\right)
$$

Claim: The distance between a point $x_{0}$ and the hyperplane defined by $w$ is $\left|w^{\top} x_{0}\right|$

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-04}
\end{center}

\section*{Linear Classifier}
Proof: The distance between $x_{0}$ and the hyperplane is given by $\min _{u: w^{\top} u=0}\left\|x_{0}-u\right\|$

Let $v=x_{0}-w^{\top} x_{0} w$ then by the Pythagorean theorem for any $u$ s.t. $w^{\top} u=0$

$\left\|x_{0}-u\right\|^{2}=\left(w^{\top} x_{0}\right)^{2}+\|v-u\|^{2} \geq\left(w^{\top} x_{0}\right)^{2}$

Claim: The distance between a point $x_{0}$ and the hyperplane defined by $w$ is $\left|w^{\top} x_{0}\right|$

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-05}
\end{center}

\section*{Hard-SVM rule: max-margin separating hyperplane}
First assume the dataset $\left(x_{n}, y_{n}\right)_{n=1}^{N}$ is linearly separable

Margin of a hyperplane: $\min _{n \leq N}\left|w^{\top} x_{n}\right|$

$$
n \leq N \quad x^{\top} w=0
$$

Max-margin separating hyperplane:

$$
\max _{w,\|w\|=1} \min _{n \leq N}\left|w^{\top} x_{n}\right| \text { such that } \forall n, y_{n} x_{n}^{\top} w \geq 0
$$

Equivalent to $\max _{M \in \mathbb{R}, w,\|w\|=1} M$ such that $\forall n, y_{n} x_{n}^{\top} w \geq M$

also equivalent to:

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-06}
\end{center}

$$
\min _{w} \frac{1}{2}\|w\|^{2} \text { such that } \forall n, y_{n} x_{n}^{\top} w \geq 1
$$

\section*{Proof of the equivalent formulations}
Claim: The following optimization problems are equivalent
$\max \min \left|w^{\top} x_{n}\right|$
$w,\|w\|=1 \quad n \leq N$
s.t. $\forall n, y_{n} x_{n}^{\top} w \geq 0$
$\max _{M \in \mathbb{R}, w,\|w\|=1} M$
s.t. $\forall n, y_{n} x_{n}^{\top} w \geq M$

Proof: Let $w_{1}$ be a solution of (I) and $M_{1}=\min _{n \leq N}\left|w_{1}^{\top} x_{n}\right|$ and let $w_{2}$ and $M_{2}$ be solutions of (II)

\begin{itemize}
  \item $\left(w_{1}, M_{1}\right)$ is admissible for (II) so $M_{1} \leq M_{2}$
  \item $w_{2}$ is admissible for (I) so $\min _{n \leq N}\left|w_{2}^{\top} x_{n}\right| \leq \min _{n \leq N}\left|w_{1}^{\top} x_{n}\right|$
  \item $\forall n, y_{n} x_{n}^{\top} w_{2} \geq M_{2}$ implies that $\forall n,\left|x_{n}^{\top} w_{2}\right| \geq M_{2}$ and $\min _{n \leq N}\left|x_{n}^{\top} w_{2}\right| \geq M_{2}$
\end{itemize}

Therefore $M_{1}=\min _{n \leq N}\left|w_{1}^{\top} x_{n}\right| \geq \min _{n \leq N}\left|w_{2}^{\top} x_{n}\right| \geq M_{2} \geq M_{1}$

And the two problems are equivalent

\section*{Proof of the equivalent formulations}
Claim: The following optimization problems are equivalent

$$
\begin{aligned}
& \max _{M \in \mathbb{R}, w,\|w\|=1} M \\
& \text { s.t. } \forall n, y_{n} x_{n}^{\top} w \geq M \\
& \min _{w} \frac{1}{2}\|w\|^{2} \\
& \text { s.t. } \forall n, y_{n} x_{n}^{\top} w \geq 1
\end{aligned}
$$

Proof:

$$
\begin{aligned}
& \max _{M \in \mathbb{R}, w,\|w\|=1} M \text { such that } \forall n, y_{n} x_{n}^{\top} w \geq M \\
\Longleftrightarrow & \max _{M \in \mathbb{R}, w} M \text { such that } \forall n, y_{n} x_{n}^{\top} \frac{w}{\|w\|} \geq M
\end{aligned}
$$

The constraints are independent of the scale of $w$. Set $\|w\|=1 / M$ :

$\Longleftrightarrow \max 1 /\|w\|$ such that $\forall n, y_{n} x_{n}^{\top} w \geq 1$

$\Longleftrightarrow \min _{w}^{w} \frac{1}{2}\|w\|^{2}$ such that $\forall n, y_{n} x_{n}^{\top} w \geq 1$

\section*{Soft SVM: a relaxation of the Hard-SVM rule that can be applied even if the training set is not linearly separable}
Idea: Maximize the margin while allowing some constraints to be violated

How: Introduce positive slack variables $\xi_{1}, \cdots, \xi_{N}$ and replace the constraints with $y_{n} x_{n}^{\top} w \geq 1-\xi_{n}$ Soft SVM:

$$
\begin{aligned}
& \min _{w, \xi} \frac{\lambda}{2}\|w\|^{2}+\frac{1}{N} \sum_{n=1}^{N} \xi_{n} \\
& \text { s.t. } \forall n, y_{n} x_{n}^{\top} w \geq 1-\xi_{n} \text { and } \xi_{n} \geq 0
\end{aligned}
$$

which is equivalent to

$$
\min _{w} \frac{\lambda}{2}\|w\|^{2}+\frac{1}{N} \sum_{n=1}^{N}\left[1-y_{n} x_{n}^{\top} w\right]_{+}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-09}
\end{center}

$$
\xi_{i}^{*}=\frac{\xi_{i}}{\|w\|}
$$

$\bullet$

\section*{Soft SVM: a relaxation of the Hard-SVM rule that can be}
 andied even if the trainina set is not linearly separableProof: Fix $w$ and consider the minimization over $\xi$ :

\begin{itemize}
  \item If $y_{n} x_{n}^{\top} w \geq 1$, then $\xi_{n}=0$
  \item If $y_{n} x_{n}^{\top} w<1, \xi_{n}=1-y_{n} x_{n}^{\top} w$
\end{itemize}

Therefore $\xi_{n}=\left[1-y_{n} x_{n}^{\top} w\right]_{+}$

$$
\begin{aligned}
& \min _{w, \xi} \frac{\lambda}{2}\|w\|^{2}+\frac{1}{N} \sum_{n=1}^{N} \xi_{n} \\
& \text { s.t. } \forall n, y_{n} x_{n}^{\top} w \geq 1-\xi_{n} \text { and } \xi_{n} \geq 0
\end{aligned}
$$

which is equivalent to

$$
\min _{w} \frac{\lambda}{2}\|w\|^{2}+\frac{1}{N} \sum_{n=1}^{N}\left[1-y_{n} x_{n}^{\top} w\right]_{+}
$$

raints to

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-10}
\end{center}

$$
\xi_{i}^{*}=\frac{\xi_{i}}{\|w\|}
$$

\section*{Classification by risk minimization}
Setting: $(X, Y) \sim \mathscr{D}$ with ranges $\mathscr{X}$ and $\mathscr{Y}=\{-1,1\}$

Goal: Find a classifier $f: \mathscr{X} \rightarrow \mathcal{Y}$ that minimizes the true risk

$$
L(f)=\mathbb{E}_{\mathscr{D}}\left(1_{Y \neq f(X)}\right)
$$

How: Through Empirical Risk Minimization (ERM):

$$
\min _{w} L_{\text {train }}(w)=\frac{1}{N} \sum_{n=1}^{N} \phi\left(y_{n} w^{\top} x_{n}\right)
$$

$\phi$ represents the loss function of the functional margin $y_{n} x_{n}^{\top} w$

$\phi$ also serves as a convex surrogate for the $0-1$ loss

\section*{Losses for Classification}
Examples of margin-based losses $\left(\eta=y x^{\top} w\right)$ :

\begin{itemize}
  \item Quadratic loss: $\operatorname{MSE}(\eta)=(1-\eta)^{2}$
  \item Logistic loss: $\operatorname{Logistic}(\eta)=\frac{\log (1+\exp (-\eta))}{\log (2)}$
  \item Hinge loss: $\operatorname{Hinge}(\eta)=[1-\eta]_{+}$
\end{itemize}

Common features: these losses are convex and provide an upper bound for the zero-one loss

Behavioral differences:

\begin{itemize}
  \item MSE: Penalizes any deviation from 1
  \item Logistic Loss: Asymmetric cost - a penalty is always incurred.
\end{itemize}

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-12}
\end{center}

\begin{itemize}
  \item Hinge Loss: A penalty is applied if the prediction is incorrect or lacks confidence
\end{itemize}

\section*{Summary}
$$
\min _{w} \frac{\lambda}{2}\|w\|^{2}+\frac{1}{N} \sum_{n=1}^{N}\left[1-y_{n} x_{n}^{\top} w\right]_{+}
$$

ERM for the hinge loss with ridge regularization

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-13(1)}
\end{center}

Interpretation for separable data with small $\lambda$ :

\begin{enumerate}
  \item Choose the direction of $w$ such that $w^{\perp}$ acts as a separating hyperplane

  \item Adjust the scale of $w$ to ensure that no point lies with the margin

  \item Select the hyperplane with the largest margin

\end{enumerate}

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-13}
\end{center}

\section*{Optimization: How to get $w$ ?}
$$
\min _{w} \frac{1}{N} \sum_{n=1}^{N}\left[1-y_{n} x_{n}^{\top} w\right]_{+}+\frac{\lambda}{2}\|w\|^{2}
$$

Convex (but non-smooth) objective which can be minimized with:

\begin{itemize}
  \item Subgradient method
  \item Stochastic Subgradient method
\end{itemize}

\section*{Convex duality}
Assume you can define an auxiliary function $G(w, \alpha)$ such that

$$
\min _{w} L(w)=\min _{w} \max _{\alpha} G(w, \alpha)
$$

Primal problem: $\min \max G(w, \alpha)$

w $\quad \alpha$

Dual problem: $\max \min G(w, \alpha)$

$\alpha \quad w$

$\Rightarrow$ Sometimes, the dual problem is easier to solve than the primal problem.

Questions:

\begin{enumerate}
  \item How do we identify a suitable $G(w, \alpha)$ ?

  \item Under what conditions can the min and max be interchanged?

  \item When is the dual problem more tractable than the primal problem?

\end{enumerate}

\section*{Q1: How do we find a suitable $G(w, \alpha)$ ?}
$$
[z]_{+}=\max (0, z)=\max _{\alpha \in[0,1]} \alpha z
$$

Therefore $\left[1-y_{n} x_{n}^{\top} w\right]_{+}=\max _{\alpha_{n} \in[0,1]} \alpha_{n}\left(1-y_{n} x_{n}^{\top} w\right)$

The SVM problem is equivalent to:

$$
\min _{w} L(w)=\min _{w} \max _{\alpha \in[0,1]^{n}} \underbrace{\frac{1}{N} \sum_{n=1}^{N} \alpha_{n}\left(1-y_{n} x_{n}^{\top} w\right)+\frac{\lambda}{2}\|w\|_{2}^{2}}_{G(w, \alpha)}
$$

The function $\mathrm{G}$ is convex in $w$ and concave in $\alpha$

\section*{Q2: Can the min and max be interchanged?}
Always true:

$$
\max _{\alpha} \min _{w} G(w, \alpha) \leq \min _{w} \max _{\alpha} G(w, \alpha)
$$

Equality if $G$ is convex in $w$, concave in $\alpha$ and the domains of $w$ and $\alpha$ are convex and compact:

$\max \min G(w, \alpha)=\min \max G(w, \alpha)$

\begin{center}
\includegraphics[max width=\textwidth]{2023_12_30_bf5d191916c1e78fa6d6g-17}
\end{center}

\section*{Q2: Can the min and max be interchanged?}
Always true:

$\max \min G(w, \alpha) \leq \min \max G(w, \alpha)$

$\alpha \quad w$

w $\quad \alpha$

Proof:

$\min G(\alpha, w) \leq G\left(\alpha, w^{\prime}\right)$ for any $w^{\prime}$

$w$

$\max \min G(\alpha, w) \leq \max G\left(\alpha, w^{\prime}\right)$ for any $w^{\prime}$

$\alpha w$

$\max \min G(\alpha, w) \leq \min \max G\left(\alpha, w^{\prime}\right)$

$\alpha \quad w$

$w^{\prime} \quad \alpha$

\section*{Application to SVM}
For SVM, the condition is met, allowing us to interchange min and max:

$$
\min _{w} L(w)=\max _{\alpha \in[0,1]^{n}} \min _{w} \frac{1}{N} \sum_{n=1}^{N} \alpha_{n}\left(1-y_{n} x_{n}^{\top} w\right)+\frac{\lambda}{2}\|w\|_{2}^{2}
$$

Minimizer computation:

$$
\mathbf{Y}=\operatorname{diag}(\mathbf{y})
$$

$\nabla_{w} G(w, \alpha)=-\frac{1}{N} \sum_{n=1}^{N} \alpha_{n} y_{n} x_{n}+\lambda w=0 \Longrightarrow w(\alpha)=\frac{1}{\lambda N} \sum_{n=1}^{N} \alpha_{n} y_{n} x_{n}=\frac{1}{\lambda N} \mathbf{X}^{\top} \mathbf{Y} \alpha$

Dual optimization problem:

$$
\begin{aligned}
\min _{w} L(w) & =\max _{\alpha \in[0,1]^{n}} \frac{1}{N} \sum_{n=1}^{N} \alpha_{n}\left(1-\frac{1}{\lambda N} y_{n} x_{n}^{\top} \mathbf{X}^{\top} \mathbf{Y} \alpha\right)+\frac{1}{2 \lambda N^{2}}\left\|\mathbf{X}^{\top} \mathbf{Y} \alpha\right\|_{2}^{2} \\
& =\max _{\alpha \in[0,1]^{n}} \frac{1^{\top} \alpha}{N}-\frac{1}{\lambda N^{2}} \alpha^{\top} \mathbf{Y} \mathbf{X} \mathbf{X}^{\top} \mathbf{Y} \alpha+\frac{1}{2 \lambda N^{2}}\left\|\mathbf{X}^{\top} \mathbf{Y} \alpha\right\|_{2}^{2} \\
& =\max _{\alpha \in[0,1]^{n}} \frac{1^{\top} \alpha}{N}-\frac{1}{2 \lambda N^{2}} \alpha^{\top} \underbrace{\mathbf{Y} \mathbf{X} \mathbf{X}^{\top} \mathbf{Y}}_{\text {PSD matrix }} \alpha
\end{aligned}
$$

\section*{Q3: Why?}
$$
\max _{\alpha \in[0,1]^{n}} \alpha^{\top} 1-\frac{1}{2 \lambda N} \alpha^{\top} \underbrace{\mathbf{Y X} \mathbf{X}^{\top} \mathbf{Y}}_{\text {PSD matrix }} \alpha
$$

\begin{enumerate}
  \item Differentiable Concave Problem: Efficient solutions can be achieved using
\end{enumerate}

\begin{itemize}
  \item Quadratic programming solvers
  \item Coordinate ascent
\end{itemize}

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Kernel Matrix Dependency: The cost function only depends on the data via the kernel matrix $K=\mathbf{X} \mathbf{X}^{\top} \in \mathbb{R}^{N \times N}$ - no dependency on $d$

  \item Dual Formulation Insight: $\alpha$ is typically sparse and non-zero exclusively for the training examples that are crucial in determining the decision boundary

\end{enumerate}

\section*{Interpretation of the dual formulation}
For any $\left(x_{n}, y_{n}\right)$, there is a corresponding $\alpha_{n}$ given by

$$
\max _{\alpha_{n} \in[0,1]} \alpha_{n}\left(1-y_{n} x_{n}^{\top} w\right)
$$

\begin{itemize}
  \item If $x_{n}$ is on the correct side and outside the margin, $1-y_{n} x_{n}^{\top} w<0$, then $\alpha_{n}=0$
  \item If $x_{n}$ is on the correct side and on the margin, $1-y_{n} x_{n}^{\top} w=0$, then $\alpha_{n} \in[0,1]$
  \item If $x_{n}$ is strictly inside the margin or or the incorrect side, $1-y_{n} x_{n}^{\top} w>0$, then $\alpha_{n}=1$
\end{itemize}

$\rightarrow$ The points for which $\alpha_{n}>0$ are referred to as support vectors

$$
\left(\alpha_{n}=0 \text { and } y_{n}=-1\right) \text { or }\left(\alpha_{n}=1 \text { and } y_{n}=1\right) \quad{ }_{w^{\top} x=-1} w^{\top} x=0
$$

\section*{The SVM hyperplane is supported by}
 the support vectors$$
\left(\alpha_{n}=0 \text { and } y_{n}=1\right) \text { or }\left(\alpha_{n}=1 \text { and } y_{n}=-1\right)
$$

$$
w=\frac{1}{\lambda N} \sum_{n=1}^{N} \alpha_{n} y_{n} x_{n}
$$

$\Rightarrow w$ does not depend on the observation $\left(x_{n}, y_{n}\right)$ if $\alpha_{n}=0$

$$
\left(\alpha_{n}=0 \text { and } y_{n}=-1\right) \text { or }\left(\alpha_{n}=1 \text { and } y_{n}=1\right)
$$

$$
w^{\top} x=-1 \quad w^{\top} x=0
$$

\section*{Recap}
\begin{itemize}
  \item Hard SVM - finds max-margin separating hyperplane $\min _{w} \frac{1}{2}\|w\|^{2}$ such that $\forall n, y_{n} x_{n}^{\top} w \geq 1$

  \item Soft SVM - relax the constraint for non-separable data

\end{itemize}

$$
\min _{w} \frac{\lambda}{2}\|w\|^{2}+\frac{1}{N} \sum_{n=1}^{N}\left[1-y_{n} x_{n}^{\top} w\right]_{+}
$$

\begin{itemize}
  \item Hinge loss can be optimized with (stochastic) sub-gradient method

  \item Duality: min max problem is equivalent to max min (convex-concave objective)

  \item Efficient solutions with quadratic programming and coordinate ascent

  \item The cost depends on the data via the kernel matrix (no dependency on $d$ )

\end{itemize}

\end{document}