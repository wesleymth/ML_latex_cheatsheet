\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{bbold}
\usepackage{mathrsfs}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}

\title{Machine Learning Course - CS-433 }

\author{}
\date{}


%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}
\maketitle
\section*{Generative Models}
Dec 13, 2023

Martin Jaggi

Last updated on: December 13, 2023

credits to Tatjana Chavdarova, Francesco D'Angelo and Lara Orlandic

$\square$

\section*{Generative Models}
Given a data sample $x$, a discriminative model aims at predicting its label $y$, hence it models the conditional distribution $p(y \mid x)$. Generative models instead model the distribution $p(x)$ defined over the datapoints $x$. Generative models models can be differentiated into two categories: explicit or implicit. In the former case, we explicitly model the probability distribution of the data, in the latter we aim at generating samples according to it.

Taxonomy. The following figure depicts the taxonomy of the existing generative methods:

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-02}
\end{center}

\begin{itemize}
  \item Implicit density models, epitomized by GANs, excel in generating high-quality, realistic samples but are often challenging to train due to mode collapse, where the model fails to capture the diversity of the data distribution.

  \item Tractable density models, such as Autoregressive models and Normalizing Flows, allow for exact likelihood computation, which can be beneficial for tasks that require likelihood evaluation and interpretability. Autoregressive models, which generate data sequentially, can be slow in sampling due to their nature. Normalizing Flows, however, enable more efficient sampling and inference by using invertible transformations, although they can be computationally intensive to train.

  \item Approximate models, including VAEs and Energy-based models, use a different strategy. VAEs, for example, optimize a lower bound on the likelihood, allowing for easier and more stable training than GANs but often resulting in less sharp samples.

  \item Diffusion models represent a hybrid approach. They iteratively convert noise into samples via a reverse Markov process and have shown promising results in generating highquality images and audio. However, they can be slow at sampling time.

\end{itemize}

In summary, the choice among generative models depends on the specific application and the trade-off between sample quality, sampling speed, and diversity [Xiao et al., 2021].

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-03}
\end{center}

\section*{Generative Adversarial Networks}
Generative Adversarial Networks

(GANs, Goodfellow et al. [2014]) are a family of implicit generative algorithms that are fast to sample from. The GAN framework is composed of two models: a generator $G$ with paramters $\boldsymbol{\theta}$ and a discriminator $D$ with parameters $\boldsymbol{\varphi}$. The former lears to generate samples from the data distribution $p_{d}$, while the latter learns to distinguish between real and fake samples. Therefore, contrary to singleobjective minimization $f: \mathcal{X} \rightarrow \mathbb{R}$, the optimization of a GAN is formulated as a differentiable two-player game where the generator $G$, and the discriminator $D$, aim at minimizing their own cost function $\mathcal{L}^{\boldsymbol{\theta}}$ and $\mathcal{L}^{\varphi}$, respectively, as follows:

$$
\begin{gathered}
\boldsymbol{\theta}^{\star} \in \underset{\boldsymbol{\theta} \in \Theta}{\arg \min } \mathcal{L}^{\boldsymbol{\theta}}\left(\boldsymbol{\theta}, \boldsymbol{\varphi}^{\star}\right) \\
\boldsymbol{\varphi}^{\star} \in \underset{\boldsymbol{\varphi} \in \Phi}{\arg \min } \mathcal{L}^{\boldsymbol{\varphi}}\left(\boldsymbol{\theta}^{\star}, \boldsymbol{\varphi}\right) .
\end{gathered}
$$

When $\mathcal{L}^{\boldsymbol{\theta}}=-\mathcal{L}^{\varphi}$ the game is called a zero-sum game and (2P-G) is a minmax problem.

\begin{enumerate}
  \item The discriminator "distinguishes" real vs. fake samples:
\end{enumerate}

$$
\begin{gathered}
\text { "real" samples } \\
x \sim p_{d}
\end{gathered}
$$

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-05(1)}
\end{center}

$p_{z}$ known "noise" distribution, e.g. $\mathcal{N}(0,1)$

$p_{d}$ the real data distribution

$D$ mapping $D: x \mapsto y \in[0,1]$,

where $y$ is an estimated probability that $x \sim p_{d}$

\begin{enumerate}
  \setcounter{enumi}{1}
  \item The generator aims at fooling the discriminator that its samples are real:
\end{enumerate}

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-05}
\end{center}

$G$ mapping $G: z \mapsto x$, such that if $z \sim p_{z}$, then hopefully $x \sim p_{d}$

$p_{g}$ the "fake" data distribution

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Objective
\end{enumerate}

$$
\min _{G} \max _{D} \underset{x \sim p_{d}}{\mathbb{E}}[\log D(x)]+\underset{z \sim p_{z}}{\mathbb{E}}[\log (1-D(G(z)))]
$$

\begin{itemize}
  \item Loss for $D$ : distinguish between $x \sim p_{g}$ and $x \sim p_{d}$ (binary classification):
\end{itemize}

$\mathscr{L}_{D}(G, D)=\max _{D} \underset{x \sim p_{d}}{\mathbb{E}}[\log D(x)]+\underset{z \sim p_{z}}{\mathbb{E}}[\log (1-D(G(z)))]$

\begin{itemize}
  \item Loss for $G$ : fool $D$ that $G(z) \sim p_{d}$ :
\end{itemize}

$$
\begin{aligned}
\mathscr{L}_{G}(G, D) & =\min _{G} \underset{z \sim p_{z}}{\mathbb{E}}[\log (1-D(G(z)))] \\
& :=(\text { in practice }) \max _{G} \underset{z \sim p_{z}}{\mathbb{E}}[\log (D(G(z)))]
\end{aligned}
$$

\begin{enumerate}
  \setcounter{enumi}{3}
  \item Theoretical Solution: The optimum is reached when $p_{g}=p_{d}$ and the optimal value is $-\log 4$ (proof in function space, see next slides).
\end{enumerate}

\section*{Alternating-GAN algorithm}
In practice, $G$ and $D$ are parametrized models (typically neural networks), and are optimized with gradient based methods.

$G$ : deep neural network $G(\mathbf{z} ; \boldsymbol{\theta})$ with parameters $\boldsymbol{\theta}$

$D$ : deep neural network $D(\mathbf{x} ; \boldsymbol{\varphi})$ with parameters $\varphi$

In most GAN implementations $G$ and $D$ have different losses $\mathcal{L}^{\theta}$ and $\mathcal{L}^{\varphi a}$, resp. In the following, we present the most commonly used algorithm for training GANs.

\section*{Algorithm 1 alternating-GAN}
Input: dataset $\mathcal{D}$, known distribution $p_{z}$, learning rate $\eta$, generator $\operatorname{loss} \mathcal{L}^{\theta}$, discriminator $\operatorname{loss} \mathcal{L}^{\varphi}$

Initialize: $D(\cdot ; \boldsymbol{\varphi}), G(\cdot ; \boldsymbol{\theta})$

for $t=1$ to $T$ do

\begin{center}
\begin{tabular}{lr}
$\mathbf{x} \sim p_{x}$ & \{sample real data\} \\
$\mathbf{z} \sim p_{z}$ & \{sample noise from $\left.p_{z}\right\}$ \\
$\boldsymbol{\varphi}=\boldsymbol{\varphi}-\eta \nabla_{\boldsymbol{\varphi}} \mathcal{L}^{\varphi}(\boldsymbol{\theta}, \boldsymbol{\varphi}, \mathbf{x}, \mathbf{z})$ & \{update $\mathrm{D}\}$ \\
$\mathbf{z} \sim p_{z}$ & $\left\{\right.$ sample noise from $\left.p_{z}\right\}$ \\
$\boldsymbol{\theta}=\boldsymbol{\theta}-\eta \nabla_{\boldsymbol{\theta}} \mathcal{L}^{\boldsymbol{\theta}}(\boldsymbol{\theta}, \boldsymbol{\varphi}, \mathbf{z})$ & \{update $\mathrm{G}\}$ \\
\end{tabular}
\end{center}

end for

Output: $\theta, \varphi$

For simplicity, in Alg. 1 we used gradient descent, however in practice GANs are often optimized using Adam [Kingma and Ba, 2015], and developing well performing minimax optimization methods is an active research area.
\footnotetext{${ }^{a}$ Although we minimize both the loses, this notation generalizes the zero-sum game, as the latter holds when $\mathcal{L}^{\theta}=-\mathcal{L}^{\varphi}:=\mathcal{L}$.
}

\section*{Conditional GAN - (CGAN)}
Many applications require generative model of a conditional probability distribution (e.g. "in-painting", segmentation, predicting the next frame of a video etc.).

GANs can also generate samples of conditional distribution, called Conditional GAN (CGAN) [Mirza and Osindero, 2014]. In CGANs both the Generator and the Discriminator are conditioned during training by some additional information, typically the class labels (but could also be images e.g. auto-generated edges of an image, conditioning on non-occluded portion-so as to generate the occluded part of the image etc).

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-08}
\end{center}

When conditioning on the class labels, typically one-hot vector representation of the class labels is used (empirically shown to perform better).

\section*{GAN architectures for images}
In the context of images synthesis, [Radford et al., 2015] propose specific architectures of the two models, named Deep Convolutional GANs - DCGAN. Notably, the generator uses transposed convolutional layer (a.k.a. fractionally strided convolutions), also informally called "Deconvolution layers" (wrongfully). Simlest way to explain these is that they "swap" the forward and the backward passes of a convolution layer: the forward transposed convolution operation can be thought of as the gradient of some convolution with respect to its input, which is usually how transposed convolutions are also implemented in practice.

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-09}
\end{center}

\section*{Discriminator}
\section*{Image to image translation}
\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-10}
\end{center}

Isola et al. (2016), (CGAN): automatically detected edges $\mapsto$ handbags
\includegraphics[max width=\textwidth, center]{2024_01_08_a381fc3992661ee7020eg-10(1)}

Isola et al. (2016), generalization of the CGAN model trained on edges $\mapsto$ photo (see previous figure) to human-drawn sketches

\section*{Conditional Generation of Images}
\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-11(1)}
\end{center}

(Picture from \href{http://blogs.nvidia.com/blog/2021/11/22/gaugan2-ai-art-demo/.)}{blogs.nvidia.com/blog/2021/11/22/gaugan2-ai-art-demo/.)}

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-11}
\end{center}

"A Style-Based Generator Architecture for Generative Adversarial Networks", CVPR 2019, https: \href{//arxiv.org/abs/1812}{//arxiv.org/abs/1812}. 04948

\section*{GAN generated images}
$512 \times 512$ samples from the class-conditional

BigGAN [Brock et al., 2019].
\includegraphics[max width=\textwidth, center]{2024_01_08_a381fc3992661ee7020eg-12}

\section*{Diffusion models}
Diffusion models are another class of generative models that have gained popularity in recent years, both for their enhanced performance and implementation in AI image generators like DALL-E 3, Stable Diffusion, and Midjourney. Unlike GANs, which train two separate models for data generation and discrimination, Diffusion Models work by progressively adding noise to input data and training one single model to estimate the added noise and recover the data.

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-13}
\end{center}

(The following content is based on this lecture)

To formalize this process, consider a Markov chain with $X_{0} \sim p_{0}$ and a forward transition such that $X_{t+1} \sim p\left(\cdot \mid X_{t}\right)$ then we call forward decomposition:

$$
p\left(x_{0: T}\right)=p_{0}\left(x_{0}\right) \prod_{t=0}^{T-1} p\left(x_{t+1} \mid x_{t}\right)
$$

where at each step, the marginal distribution of $X_{t}$ satisfies:

$$
p\left(x_{t}\right)=\int p\left(x_{t} \mid x_{t-1}\right) p\left(x_{t-1}\right) \mathrm{d} x_{t-1}
$$

The backward transition can be obtained with Bayes'rule $p\left(x_{t} \mid x_{t+1}\right)=$ $p\left(x_{t+1} \mid x_{t}\right) p\left(x_{t}\right) / p\left(x_{t+1}\right)$, and we call backward decomposition:

$$
p\left(x_{0: T}\right)=p_{T}\left(x_{T}\right) \prod_{t=0}^{T-1} p\left(x_{t} \mid x_{t+1}\right)
$$

A generative model can be constructed sampling from $p\left(x_{0: T}\right)$ using ancestral sampling:

Sample $X_{T} \sim p_{T}(\cdot)$ then:

for $k=T-1, \ldots, 0$ :

Sample $\quad X_{t} \sim p\left(\cdot \mid X_{t+1}\right)$

\begin{itemize}
  \item We consider $p_{0}=p_{\text {data }} \in \mathcal{P}\left(\mathbb{R}^{d}\right)$

  \item We chose the forward transition to be Gaussian $p\left(x_{t+1} \mid x_{t}\right)=\mathcal{N}\left(x_{t+1} ; \alpha x_{t},(1-\right.$ $\left.\left.\alpha^{2}\right) \mathbb{I}_{d}\right)$

  \item The conditional $p\left(x_{t} \mid x_{0}\right)$ is also Gaussian $\mathcal{N}\left(x_{t} ; \alpha^{t} x_{0},\left(1-\alpha^{2 t}\right) \mathbb{I}_{d}\right)$

  \item The Markov chain converges to $p_{\text {ref }}=$ $\mathcal{N}\left(x ; 0, \mathbb{I}_{d}\right)$ for $T \rightarrow \infty$.

  \item Therefore for $\mathrm{T}$ large enough $p_{T}(x) \approx$ $p_{\text {ref }}(x)$.

  \item To generate samples we use ancestral sampling replacing $p_{T}$ with $p_{\text {ref. }}$.

\end{itemize}

Key problem: the backward transition $p\left(x_{t} \mid x_{t+1}\right)$ can't be computed, we need an approximation!

Using a Taylor expansion we can approximate the backward transition:

$$
\begin{aligned}
& p\left(x_{t} \mid x_{t+1}\right)=p\left(x_{t+1} \mid x_{t}\right) \exp \left[\log p\left(x_{t}\right)-\log p\left(x_{t+1}\right)\right] \\
& \approx \mathcal{N}(x_{t} ;(2-\alpha) x_{t+1}+\left(1-\alpha^{2}\right) \underbrace{\nabla \log p\left(x_{t+1}\right)}_{\text {Score }},\left(1-\alpha^{2}\right) \mathbb{I}_{d}) .
\end{aligned}
$$

\section*{Denoising score matching}
The Stein Score is analytically intractable,

but we can approximate it using a neural network $s_{\theta}\left(x_{t}\right)$. Surprisingly, we can learn the score solving a simple regression problem via the Denoising Score Matching (DSM) loss:

$\theta^{\star}=\underset{\theta}{\arg \min } \mathcal{L}(\theta):=\frac{1}{2} \mathbb{E}_{X_{t}}\left[\left\|\nabla \log p\left(X_{t}\right)-s_{\theta}\left(X_{t}\right)\right\|^{2}\right]$

(DSM-Loss)

This problem is still intractable, we do not have access to the marginal $p\left(x_{t}\right)$ at each step but we can manipulate it:

$$
\begin{aligned}
\mathcal{L}(\theta) & =\mathrm{C}_{1}+\frac{1}{2} \mathbb{E}_{X_{t}}\left[\left\|s_{\theta}\left(X_{t}\right)\right\|^{2}\right]-\mathbb{E}_{X_{t}}\left[\nabla \log p\left(X_{t}\right)^{\top} s_{\theta}\left(X_{t}\right)\right] \\
& =\mathrm{C}_{1}+\frac{1}{2} \mathbb{E}_{X_{t}}\left[\left\|s_{\theta}\left(X_{t}\right)\right\|^{2}\right]-\int \nabla \log p\left(X_{t}\right)^{\top} s_{\theta}\left(X_{t}\right) p\left(X_{t}\right) \mathrm{d} x_{t}
\end{aligned}
$$

Using that $p\left(x_{t}\right)=\int p_{0}\left(x_{0}\right) p\left(x_{t} \mid x_{0}\right) \mathrm{d} x_{0}$ we get:

$$
\nabla \log p\left(X_{t}\right)=\mathbb{E}_{X_{0} \mid X_{t}}\left[\nabla \log p\left(X_{t} \mid X_{0}\right)\right]
$$

Substituding in the DSM-loss:

$$
\begin{aligned}
\mathcal{L}(\theta) & =\mathrm{C}_{1}+\frac{1}{2} \mathbb{E}_{X_{t}}\left[\left\|s_{\theta}\left(X_{t}\right)\right\|^{2}\right]-\mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\nabla \log p\left(X_{t} \mid X_{0}\right)^{\top} s_{\theta}\left(X_{t}\right)\right] \\
& =\mathrm{C}_{1}+\frac{1}{2} \mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|s_{\theta}\left(X_{t}\right)-\nabla \log p\left(X_{t} \mid X_{0}\right)\right\|^{2}\right] \\
& -\frac{1}{2} \underbrace{\mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|\nabla \log p\left(X_{t} \mid X_{0}\right)\right\|^{2}\right]}_{\text {const. }} \\
& =\mathrm{C}_{2}+\frac{1}{2} \mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|s_{\theta}\left(X_{t}\right)-\nabla \log p\left(X_{t} \mid X_{0}\right)\right\|^{2}\right]
\end{aligned}
$$

In practice we estimate all the scores simultaneously i.e. $s_{\theta^{\star}}\left(t, X_{t}\right)$ such that:

$$
\theta^{\star}=\underset{\theta}{\arg \min } \frac{1}{2} \sum_{t=1}^{T} \mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|s_{\theta}\left(t, X_{t}\right)-\nabla \log p\left(X_{t} \mid X_{0}\right)\right\|^{2}\right]
$$

Recalling now our choice of forward transition for which $p\left(x_{t} \mid x_{0}\right)=\mathcal{N}\left(x_{t} ; \alpha^{t} x_{0},(1-\right.$ $\left.\left.\alpha^{2 t}\right) \mathbb{I}_{d}\right)$ :

\begin{itemize}
  \item $\nabla \log p\left(x_{t} \mid x_{0}\right)=-\frac{\left(x_{t}-\alpha^{t} x_{0}\right)}{\left(1-\alpha^{2 t}\right)}$

  \item $X_{t}=\alpha^{t} X_{0}+\sqrt{1-\alpha^{2 t}} \xi_{t} \xi_{t} \sim \mathcal{N}\left(0, \mathbb{I}_{d}\right)$

  \item $\nabla \log p\left(X_{t} \mid X_{0}\right)=-\frac{\xi_{t}}{\sqrt{1-\alpha^{2 t}}}$

\end{itemize}

If we parameterize the score function as $s_{\theta}\left(t, X_{t}\right)=-\frac{\hat{\xi}_{\theta}\left(t, X_{t}\right)}{\sqrt{1-\alpha^{2 t}}}$ then the problem is equivalent to:

$$
\theta^{\star}=\underset{\theta}{\arg \min } \frac{1}{2} \sum_{k=1}^{T} \mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|\hat{\xi}_{\theta}\left(t, X_{t}\right)-\xi_{t}\right\|^{2}\right]
$$

which has the illuminating interpretation of learning to predict the added noise $\xi_{t}$ from the noised data $X_{t}$.

Finally, we can generate new samples from the backward process by sampling $X_{T} \sim p_{\text {ref }}$ and using the approximation of the backward transition and the learned score to perform ancestral sampling:

$$
X_{t}=(2-\alpha) X_{t+1}+\left(1-\alpha^{2}\right) s_{\theta^{\star}}\left(t+1, X_{t+1}\right)+\sqrt{1-\alpha^{2}} \xi_{t+1} \quad \xi_{t+1} \sim \mathcal{N}\left(0, \mathbb{I}_{d}\right)
$$

\section*{Implementation}
Diffusion models often use a U-net architecture with ResNet blocks and self attention layers to represent the score function $\hat{\xi}_{\theta}\left(t, X_{t}\right)$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-17}
\end{center}

(The above figure is taken from this tutorial)

The time step can be represented with sinusoidal positional encodings as done for transformers or with random Fourirer features. The time features are then fed to the residual blocks using either spatial addition or adaptive group normalization layers Dhariwal and Nichol [2021].

\section*{Training and Sampling}
\section*{Algorithm 2 Training}
Input: Data distribution $p_{0}, \alpha$

Output: Learned parameters $\theta^{*}$

Initialize: Parameters $\theta$

repeat

Sample initial data $X_{0} \sim p_{0}$

Sample time step $t \sim \operatorname{Uniform}(\{1, \ldots, T\})$

Sample noise $\xi_{t} \sim \mathcal{N}\left(0, \mathbb{I}_{d}\right)$

Update $\theta$ using gradient descent step on:

$\nabla_{\theta} \frac{1}{2} \mathbb{E}_{X_{0}} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|s_{\theta}\left(t, \alpha^{t} X_{0}+\sqrt{1-\alpha^{2 t}} \xi_{t}\right)-\xi_{t}\right\|^{2}\right]$ until convergence

\section*{Algorithm 3 Sampling}
Input: Learned parameters $\theta^{*}, p_{\text {ref }}, \alpha$

Output: Generated sample $X_{0}$

Sample $X_{T} \sim p_{\text {ref }}$

for $t=T-1$ to 0 do

Sample noise $\xi_{t+1} \sim \mathcal{N}\left(0, \mathbb{I}_{d}\right)$ :

$$
\begin{aligned}
& X_{t}=(2-\alpha) X_{t+1}+\left(1-\alpha^{2}\right) s_{\theta^{\star}}\left(t+1, X_{t+1}\right)+ \\
& \sqrt{1-\alpha^{2}} \xi_{t+1}
\end{aligned}
$$

end for

\section*{Exponential moving average}
The training of the U-net via Denoising Score Matching can be very unstable. To regularize it, usually an exponential moving average of the weights is used along the training:

$$
\bar{\theta}_{n+1}=(1-\beta) \bar{\theta}_{n}+\beta \theta_{n}
$$

Where the parameter $\beta$ regulates the rate of forgetting of the initial conditions.

\section*{Conditional training}
Has we have seen already for GANs, it is often useful to condition the generative model on some additional information $y$ (e.g. a class label or a text). In the case of diffusion models, the conditioning is done by adding an additional input to the score function such that a conditional score is learned:

$\theta^{\star}=\underset{\theta}{\arg \min } \frac{1}{2} \sum_{k=1}^{T} \mathbb{E}_{X_{0}, Y} \mathbb{E}_{X_{t} \mid X_{0}}\left[\left\|s_{\theta}\left(t, X_{t}^{Y} ; Y\right)-\nabla \log p\left(X_{t}^{Y} \mid X_{0}^{Y}\right)\right\|^{2}\right]$

The conditioning is fed into the network in

a similar way than the time features.

\section*{Diffusion generated images}
\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-19(2)}
\end{center}

An illustration of an avocado sitting in a therapist's chair saying 'I just feel so empty inside' with a pit-sized hole in its center. The therapist, a spoon, scribbles notes

An expressive oil painting of a basketball payer dunking, depicted as an explosion of a nebula.

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-19(1)}
\end{center}

A portrait of a dog in a library, Sigma $85 \mathrm{~mm} \mathrm{f} /$ 1.4

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-19}
\end{center}

An astronaut riding a horse

(The above figures are taken from \href{https://openai.com/dall-e-3}{https://openai.com/dall-e-3},

\href{https://openart.ai/}{https://openart.ai/} and \href{https://the-decoder.com}{https://the-decoder.com}.)

\section*{Applications of GANs and Diffusion Models}
\begin{itemize}
  \item Image Generation and Editing

  \item Art Creation

  \item Data Augmentation

  \item Adversarial Examples

  \item Drug Discovery

  \item Protein Structure Prediction

  \item Anomaly Detection

  \item Weather Forecasting

  \item Video Generation

  \item Voice Synthesis and Modification

  \item Restoration of Old Photographs and Artworks

  \item Fashion Design

  \item Architectural Visualization

  \item Gaming

  \item Virtual Reality and Augmented Reality

  \item Language Translation and Interpretation

  \item Financial Modeling

\end{itemize}

GANs and diffusion models have also been used for other data modalities: For raw-waveform audio synthesis, examples include WaveGAN [Donahue et al., 2019] and MelGAN [Kumar et al., 2019], among others. For generating realistic tabular data, see e.g. [Kotelnikov et al., 2022].

\section*{Summary}
We have studied:

\begin{enumerate}
  \item Generative models

  \item Generative Adversarial Networks

\end{enumerate}

\begin{itemize}
  \item Players (generator \& discriminator)
  \item Objectives
  \item Solution \& Algorithm, Conditional GANs
\end{itemize}

\begin{enumerate}
  \setcounter{enumi}{2}
  \item Diffusion models
\end{enumerate}

\begin{itemize}
  \item Ancestral sampling
  \item Denoising Score Matching
  \item Conditional training
\end{itemize}

\section*{Additional Notes (optional material) 
 $\mathrm{KL}$ and JS divergences}
Before proving that at the equilibrium of the above GAN framework $p_{g}=p_{d}$ we need to define some measures of similarity between two probability distributions: The KL and JS divergences.

The Kullback-Leibler (KL) divergence is defined as:

$$
\mathbb{D}_{K L}\left(p_{d} \| p_{g}\right):=\int_{x} \log \left(\frac{p_{d}(x)}{p_{g}(x)}\right) p_{d}(x) \mathrm{d} x .
$$

KL is also called relative entropy, as it measures how one probability distribution is different from a "reference" probability distribution, and it is asymmetric.

The Jenson-Shannon (JS) divergence is defined as:

$\mathbb{D}_{J S}(p \| q):=\frac{1}{2} \mathbb{D}_{K L}\left(p \| \frac{p+q}{2}\right)+\frac{1}{2} \mathbb{D}_{K L}\left(q \| \frac{p+q}{2}\right)$

Note that contrary to the KL divergence defined above, the JS divergence is symmetric.

\section*{The GAN framework: 
 Equilibrium at $p_{g}=p_{d}$}
In the following, we'll assume the neural network models $\mathrm{G}$ and $\mathrm{D}$ have infinite capacity, so can represent any probability distribution. We will study the convergence of the loss function in the space of probability density functions.

The discriminator maximizes:

$$
\begin{aligned}
\mathcal{L}(G, D) & =\int_{x} p_{d}(x) \log (D(x)) \mathrm{d} x+\int_{z} p_{z}(z) \log (1-D(G(z))) \mathrm{d} z \\
& =\int_{x} p_{d}(x) \log (D(x))+p_{g}(x) \log (1-D(x)) \mathrm{d} x
\end{aligned}
$$

Where we used $x=G(z)$, and $p_{g}$ is the distribution of $x$.

The above integrand can be written as: $f(y)=a \log y+b \log (1-y)$. To solve for its critical points: $f^{\prime}(y)=0 \Rightarrow \frac{a}{y}-\frac{b}{1-y}=$ $0 \Rightarrow y=\frac{a}{a+b}$. Moreover, if $a+b \neq 0$ we obtain that $\frac{a}{a+b}$ is a maximum as $f^{\prime \prime}\left(\frac{a}{a+b}\right)<0$. Hence, optimal discriminator $D^{\star}$ is:

$$
D^{\star}(x)=\frac{p_{d}(x)}{p_{d}(x)+p_{g}(x)}
$$

By replacing the optimal discriminator in the above objective, we obtain that the generator minimizes:

$$
\begin{aligned}
\mathcal{L}\left(G, D^{\star}\right) & =\underset{x \sim p_{d}}{\mathbb{E}}\left[\log D^{\star}(x)\right]+\underset{x \sim p_{g}}{\mathbb{E}}\left[\log \left(1-D^{\star}(x)\right)\right] \\
& =\underset{x \sim p_{d}}{\mathbb{E}}\left[\log \frac{p_{d}(x)}{p_{d}(x)+p_{g}(x)}\right]+\underset{x \sim p_{g}}{\mathbb{E}}\left[\log \frac{p_{g}(x)}{p_{d}(x)+p_{g}(x)}\right] \\
& =-\log 4+\mathbb{D}_{K L}\left(p_{d} \| \frac{p_{d}+p_{g}}{2}\right)+\mathbb{D}_{K L}\left(p_{g} \| \frac{p_{d}+p_{g}}{2}\right) \\
& =-\log 4+2 \cdot \mathbb{D}_{J S}\left(p_{d} \| p_{g}\right)
\end{aligned}
$$

where to obtain the third expression we used the definition of logarithm:

$$
\begin{aligned}
& \log 2+\log \left(\frac{p_{d}(x)}{p_{g}(x)+p_{d}(x)}\right) \\
& =\log \left(2 \frac{p_{d}(x)}{p_{g}(x)+p_{d}(x)}\right) \\
& =\log \left(\frac{p_{d}(x)}{\frac{p_{g}(x)+p_{d}(x)}{2}}\right) .
\end{aligned}
$$

Above, $\mathbb{D}_{K L}$ and $\mathbb{D}_{J S}$ again denote the Kullback-Leibler and the Jenson-Shannon divergences (see previous slides).

The optimum is reached when $p_{g}=p_{d}\left(\right.$ note $\left.D^{\star}=\frac{1}{2}\right)$, and the optimal value is $-\log 4$.

\section*{Drawback of using JS divergence for GANs}
Example: Let us consider two probability distributions defined on $\mathbb{R}: P: \delta_{0}(x)$ and $Q: \delta_{\theta}(x)$.

\begin{center}
\includegraphics[max width=\textwidth]{2024_01_08_a381fc3992661ee7020eg-25}
\end{center}

Note that when the supports of the two distributions are disjoint $(\theta \neq 0)$, we obtain $\mathbb{D}_{K L}(P \| Q)=+\infty$, and $\mathbb{D}_{J S}(P \| Q)=\log 2$, both yielding nonsmooth gradient, making gradient-based methods hard to use [Arjovsky et al., 2017].

\section*{Wasserstein Distance}
The previous example motivates the use of the Wasserstein distance (aka Earth mover's distance) in the context of GANs, described next.

Wasserstein-1 distance is defined as:

$$
\mathbb{D}_{W}\left(p_{d}, p_{g}\right)=\inf _{\gamma \sim \Pi\left(p_{d}, p_{g}\right)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|],
$$

where $\Pi\left(p_{d}, p_{g}\right)$ is the set of all possible joint probability distributions between $p_{d}$ and $p_{g}$, whose marginals are $p_{d}, p_{g}$, resp.

Intuitively, the two distributions can be viewed as a mass on each point. The goal is to move these masses so that one distribution can be transformed into the other. As there are infinitely many ways of doing so, $\mathbb{D}_{W}\left(p_{d}, p_{g}\right)$ is the minimum cost we need to spend. The cost is the amount of mass that has to be transported times the distance it has to be moved.

Note that in the above example $\mathbb{D}_{W}\left(\delta_{0}(x), \delta_{\theta}(x)\right)=|\theta|$, and it provides "usable" gradient. However, $\mathbb{D}_{W}(\cdot)$ does not scale with the dimensionality of the input random variables, as the number of states of the joint probability distribution grows exponentially. Fortunately, it can be alternatively formalized (see additional material).

\section*{Wasserstein GAN}
Def. $f: \mathbb{R} \rightarrow \mathbb{R}$ is called $k$-Lipschitz continuous if $\exists k \in \mathbb{R}$ s.t.

$$
\left|f\left(x_{1}\right)-f\left(x_{2}\right)\right| \leq k\left|x_{1}-x_{2}\right|, \quad \forall x_{1}, x_{2} .
$$

The so called Wasserstein GAN [WGAN, Arjovsky et al., 2017, Gulrajani et al., 2017] replaces the JS divergence with the Wasserstein distance. As the Wasserstein distance is intractable for Deep Neural Nets, WGANs make use of the so called Kantorovich-Rubinstein duality principle, which tells us that:

$\mathbb{D}_{W}\left(p_{d}, p_{g}\right)=\sup _{\|f\|_{L} \leq 1} \underset{x \sim p_{d}}{\mathbb{E}}[f(x)]-\underset{x \sim p_{g}}{\mathbb{E}}[f(x)]$,

where the supremum is over 1-Lipschitz functions $f: \mathcal{X} \mapsto \mathbb{R}$. Its derivation is out of the scope of this course (if interested see proof sketch in the Appendix).

In the context of GANs, $f$ is the function represented by the discriminator (called critic in WGAN), yielding:

$$
\min _{G} \max _{D \in \mathcal{D}} \underset{x \sim p_{d}}{\mathbb{E}}[D(x)]-\underset{x \sim p_{g}}{\mathbb{E}}[D(x)],
$$

where $\mathcal{D}$ is the set of 1-Lipschitz functions (see next slide).

\section*{Some GAN variants with Lipschitz Discriminator}
(optional material)

The constraint that the discriminator should be 1-Lipschitz can be enforced in several ways. The table below summarizes some of the GAN variants which enforce such constraint.

WGAN [Arjovsky et al., 2017] uses straightforward weight clipping. [Gulrajani et al., 2017] point out that this may lead to optimization difficulties, and proposed adding an extra penalty term to the training loss of the Discriminator, which penalizes gradients whose norm is higher than 1. As enforcing this for any input is intractable, this is done by considering a line between fake and real samples, obtained by interpolating between these. [Kodali et al., 2017] penalize gradients whose norm is higher than 1 while considering samples in a region around real data points. Note that $D R A G A N \& W G A N-G P$ are computationally more expensive as each parameter update requires computation of a second-order derivative (as the loss includes gradient penalty). [Miyato et al., 2018] make use of the power iteration method to estimate the largest singular value per layer, so as to divide the parameters of that layer with it. In the context of training GANs, although the power iteration method requires multiple iterations to compute the largest singular value, it cam be implemented efficiently due to the fact that it is combined with Stochastic gradient based method. In other words, it was shown in practice that one update of it per stochastic parameter update is sufficient to obtain good estimates of the largest singular value. It was empirically shown that enforcing 1-Lipschitz Discriminator helps the convergence of GAN trained with JS-based losses, and JS-based GANs remained widely used in practice. Moreover, besides lacking theoretical backup, one of the well performing large-scale GAN variants further enforces that both the Generator and the discriminator are 1-Lipschitz functions.

\begin{center}
\begin{tabular}{cccc}
\hline
 & $\mathbb{D}_{-}\left(p_{d} \| p_{g}\right)$ & $G / D$ & Mean of enforcing Lipschitz continuity \\
\hline
\begin{tabular}{c}
$W G A N$ \\
Arjovsky et al., 2017] \\
$W G A N-G P$ \\
\end{tabular} & $\mathbb{D}_{W}$ & $D$ & Weight clipping: $w=\operatorname{clip}(w,-c, c)$ \\
\begin{tabular}{c}
Gulrajani et al., 2017] \\
DRAGAN \\
\end{tabular} & $\mathbb{D}_{W}$ & $D$ & Gradient penalty: line between real and fake points \\
\begin{tabular}{c}
Kodali et al., 2017] \\
SNGAN \\
\end{tabular} & $\sim \mathbb{D}_{J S D}$ & $D$ & Gradient penalty: around real data points \\
\begin{tabular}{c}
Miyato et al., 2018] \\
bigGAN \\
\end{tabular} & $\sim \mathbb{D}_{J S D}$ & $D$ & Spectral norm using the power iteration method \\
% [Brock et al., 2019]
 & $\sim \mathbb{D}_{J S D}$ & $G \& D$ & Spectral norm using the power iteration method \\
\hline
\end{tabular}
\end{center}

\section*{Appendix A: Wasserstein Distance}
\section*{Continious probability distributions}
The Wasserstein distance between two probability distributions $\mu$ and $\nu$ is defined as:

$$
W(\mu, \nu)=\inf _{\pi(\mu, \nu)} \iint c(x, y) \pi(d x, d y)
$$

Equation (1) assumes continuous distributions $\mu$ and $\nu$. If we use the Euclidean distance we have:

$$
\begin{gathered}
W(\mu, \nu)=\inf _{\pi} \iint\|x-y\| \pi(x, y) \mathrm{d} x \mathrm{~d} y \\
=\inf _{\pi} \mathbb{E}_{x, y \sim \pi}[\|x-y\|] .
\end{gathered}
$$

\section*{Discrete probability distributions}
Lets consider two discrete distributions $\mathbb{P}_{x}, \mathbb{P}_{y}$, with $s$ states each: $x_{i}$ and $y_{i}, i=1 \ldots s$. Thus,

$$
\begin{aligned}
W\left(\mathbb{P}_{x}, \mathbb{P}_{y}\right) & =\inf _{\Pi\left(\mathbb{P}_{x}, \mathbb{P}_{y}\right)} \sum_{x, y}\|x-y\| \Pi(x, y) \\
& =\inf _{\Pi\left(\mathbb{P}_{x}, \mathbb{P}_{y}\right)} \mathbb{E}_{(x, y) \sim \Pi[\|x-y\|]} \\
& =\inf _{\Pi\left(\mathbb{P}_{x}, \mathbb{P}_{y}\right)}\langle\Pi, D\rangle,
\end{aligned}
$$

where with $\langle\cdot, \cdot\rangle$ we denote sum of element-wise multiplication, and $\Pi \in$ $\mathbb{R}^{s \times s}$ is the joint probability and $D \in \mathbb{R}^{s \times s}$ is the Euclidean distance between each $x_{i}, y_{j}, i=1 \ldots s, j=1 \ldots s$.

\section*{Kantorovich-Rubinstein dualty principle}
From Kantorovich-Rubinstein dualty [Villani, 2008]:

$$
\mathbb{D}_{W}\left(p_{d}, p_{g}\right)=\frac{1}{K} \sup _{\|f\|_{L} \leq K} \underset{x \sim p_{d}}{\mathbb{E}}[f(x)]-\underset{x \sim p_{g}}{\mathbb{E}}[f(x)]
$$

Villani [2008] gives the following intuitive interpretation of the above Kantorovich duality principle. Namely, if our goal is to transfer a huge
amount of mass distributed over certain area, to a different distinct area, we would like to minimize the cost for transport, thus we have an inf over the implied cost. Suppose we have a middle-man who offers to handle the problem for us, by claiming that he will not charge us more than the actual transport cost (thus $\varphi(y)-\psi(x) \leq c(x, y)$ ). Then, our initial problem is in fact equal to the one of the middle-man trying to maximize his profit. His profit on the other hand is defined as a price for loading goods: $\varphi(x)$ and a price for unloading them at destination $y$ : $\psi(y)$. Naturally, he aims at maximizing his profit (thus the sup). Note however that the middle man is ready to give financial compensations for some places, in the form of negative prices.

Proof sketch. See [Villani, 2008] for full formal proof.

$x$ cont. r.v.

$\mu$ distribution of $x$

$\nu$ target distribution

$c(\cdot)$ cost, e.g. $\ell_{p}$ norm

$$
\begin{aligned}
\mathbb{D}_{W}(\mu, \nu) & =\inf _{\gamma \in \pi} \iint c(x, y) \mathrm{d} \gamma(x, y) \\
& =\sup _{\varphi, \psi \rightarrow \mathbb{R}} \int \varphi(y) \mathrm{d} \nu-\psi(x) \mathrm{d} \mu
\end{aligned}
$$

where $\varphi(y)-\psi(x) \leq c(x, y)$, and $\pi$ is set of all non-negative Borel measures, whose marginals are $\int_{x} \pi=\nu$ and $\int_{y} \pi=\mu$.

$$
\begin{aligned}
\mathbb{D}_{W}(\mu, \nu) & =\inf _{\gamma \in \pi} \iint c(x, y) \mathrm{d} \gamma(x, y) \\
& =\inf _{\gamma \in \pi} \iint c(x, y) \mathrm{d} \gamma(x, y)+ \begin{cases}0, & \text { if } \gamma \in \pi \\
-\infty, & \text { otherwise }\end{cases} \\
& =\inf _{\gamma \in \pi}\left\{\int c(x, y) \mathrm{d} \gamma(x, y)+\sup _{\varphi, \psi \rightarrow \mathbb{R}}\left[\int \varphi(y) \mathrm{d} \nu-\int \psi(x) \mathrm{d} \mu-\iint(\varphi(y)-\psi(x)) \mathrm{d} \gamma(x, y)\right]\right\} \\
& =\inf _{\gamma \in \pi} \sup _{\varphi, \psi \rightarrow \mathbb{R}}\left\{\int \varphi(y) \mathrm{d} \nu-\int \psi(x) \mathrm{d} \mu+\iint(c(x, y)-(\varphi(y)-\psi(x))) \mathrm{d} \gamma(x, y)\right\} \\
& =\sup _{\varphi, \psi \rightarrow \mathbb{R}}\left\{\int \varphi(y) \mathrm{d} \nu-\int \psi(x) \mathrm{d} \mu+\inf _{\gamma \in \pi} \iint(c(x, y)-(\varphi(y)-\psi(x))) \mathrm{d} \gamma(x, y)\right\} \\
& =\sup _{\varphi, \psi \rightarrow \mathbb{R}}\left\{\int \varphi(y) \mathrm{d} \nu-\int \psi(x) \mathrm{d} \mu+ \begin{cases}0, & \text { if } \varphi(y)-\psi(x) \leq c(x, y) \\
-\infty, & \text { otherwise }\end{cases} \right. \\
& =\sup _{\varphi, \psi \rightarrow \mathbb{R}}\left\{\int \varphi(y) \mathrm{d} \nu-\int \psi(x) \mathrm{d} \mu\right\} .
\end{aligned}
$$

\section*{References}
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In ICML, 2017.

Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In ICLR, 2019.

Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.

Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In ICLR, 2019.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein GANs. In NIPS, 2017.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In $I C L R, 2015$.

Naveen Kodali, Jacob D. Abernethy, James Hays, and Zsolt Kira. How to train your DRAGAN. arXiv:1705.07215, 2017.

Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. arXiv preprint arXiv:2209.15421, 2022.

K. Kumar, Rithesh Kumar, T. D. Boissière, L. Gestin, Wei Zhen Teoh, J. Sotelo, A. D. Brébisson, Yoshua Bengio, and Aaron C. Courville. Melgan: Generative adversarial networks for conditional waveform synthesis. In NeurIPS, 2019.

Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In $I C L R, 2018$.

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv:1511.06434, 2015.

Cédric Villani. Optimal Transport: Old and New. Springer, 2009 edition, September 2008. ISBN 3540710493.

Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021.


\end{document}