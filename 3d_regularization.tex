\section{Regularization: $
\min _{\mathbf{w}} \mathcal{L}(\mathbf{w})+\Omega(\mathbf{w})
$}

\subsection*{Ridge Regression: $L_{2}$-Regularization + $\mathcal{L}_{MSE}$}
- Euclidean norm $(L_{2}$ norm) $
\Omega(\mathbf{w})=\lambda\|\mathbf{w}\|_{2}^{2}
$, $\|\mathbf{w}\|_{2}^{2}=\sum_{i} w_{i}^{2}$. 

$\min _{\mathbf{w}} \frac{1}{2 N} \sum_{n=1}^{N}[y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}]^{2}+\lambda\|\mathbf{w}\|_{2}^{2}
=\min _{w} \frac{1}{2 N}\|\mathbf{y}-\mathbf{X} w\|^{2}+\lambda\|w\|^{2}_{2}
$

% - Least squares is a special case of this: set $\lambda:=0$.

\subsection*{Explicit solution for w:}

$\nabla = \nabla \mathcal{L} + \nabla\Omega = -\frac1NX^T(y-Xw) + \lambda2w = 0 \\\Rightarrow \mathbf{w}_{\text {ridge }}^{\star}=(\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I})^{-1} \mathbf{X}^{\top} \mathbf{y}$ , ($\frac{\lambda^{\prime}}{2 N}=\lambda$ )

\subsection*{Ridge Regression Fights Ill-Conditioning}
- Lifting the eigenvalues : The eigenvalues of $(\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I})$ are all at least $\lambda^{\prime}$ and so the inverse always exists.

- Proof: Write the Eigenvalue decomposition of $\mathbf{X}^{\top} \mathbf{X}$ as $\mathbf{U S U}^{\top}$,
$
\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I} =\mathbf{U S} \mathbf{U}^{\top}+\lambda^{\prime} \mathbf{U I U}^{\top} =\mathbf{U}[\mathbf{S}+\lambda^{\prime} \mathbf{I}] \mathbf{U}^{\top}
$


% - Alternative proof: Recall that for a symmetric matrix $\mathbf{A}$ we can also compute eigenvalues by looking at the so-called Rayleigh ratio,
% $
% R(\mathbf{A}, \mathbf{v})=\frac{\mathbf{v}^{\top} \mathbf{A} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}
% $

% Note that if $\mathbf{v}$ is an eigenvector with eigenvalue $\lambda$ then the Rayleigh coefficient indeed gives us $\lambda$. We can find the smallest and largest eigenvalue by minimizing and maximizing this coefficient. But note that if we apply this to the symmetric matrix $\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I}$ then for any vector $\mathbf{v}$ we have
% $
% \frac{\mathbf{v}^{\top}(\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I}) \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}} \geq \frac{\lambda^{\prime} \mathbf{v}^{\top} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}=\lambda^{\prime}
% $

\subsection*{$L_{1}$-Regularization: The Lasso ($L_{1}$-norm + $\mathcal{L}_{\text{MSE}}$)}
$\min _{\mathbf{w}} \frac{1}{2 N} \sum_{n=1}^{N}[y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}]^{2}+\lambda\|\mathbf{w}\|_{1}$
where $
\|\mathbf{w}\|_{1}:=\sum_{i}|w_{i}|
$

% - $w_*$ is likely sparse (few non-zero components) $\rightarrow$ simpler model

% \subsection*{Geometric Interpretation}

% - Assume that $\mathbf{X}^{\top} \mathbf{X}$ is invertible. We claim that in this case the set $
% \{\mathbf{w}:\|\mathbf{y}-\mathbf{X} \mathbf{w}\|^{2}=\alpha\}
% $ is an ellipsoid and this ellipsoid simply scales around its origin as we change $\alpha$. 

% - First look at $\alpha=\|\mathbf{X w}\|^{2}=\mathbf{w}^{\top} \mathbf{X}^{\top} \mathbf{X w}$. This is a quadratic form. Let $\mathbf{A}=\mathbf{X}^{\top} \mathbf{X}$. Note that $\mathbf{A}$ is a symmetric matrix and by assumption it has full rank. If $\mathbf{A}$ is a diagonal matrix with strictly positive elements $a_{i}$ along the diagonal then this describes the equation 
% $
% \sum_{i} a_{i} \mathbf{w}_{i}^{2}=\alpha
% $ 
% which is indeed the equation for an ellipsoid. In the general case, $\mathbf{A}$ can be written as (using the SVD) $\mathbf{A}=\mathbf{U B U}^{T}$, where $\mathbf{B}$ is a diagonal matrix with strictly positive entries. This then corresponds to an ellipsoid with rotated axes. If we now look at $\alpha=\|\mathbf{y}-\mathbf{X} \mathbf{w}\|^{2}$, where $\mathbf{y}$ is in the column space of $\mathbf{X}$ then we can write it as $\alpha=\|\mathbf{X}(\mathbf{w}_{0}-\mathbf{w})\|^{2}$ for a suitable chosen $\mathbf{w}_{0}$ and so this corresponds to a shifted ellipsoid. Finally, for the general case, write $\mathbf{y}$ as $\mathbf{y}=\mathbf{y}_{\|}+$ $\mathbf{y}_{\perp}$, where $\mathbf{y}_{\|}$is the component of $\mathbf{y}$ that lies in the subspace spanned by the columns of $\mathbf{X}$ and $\mathbf{y}_{\perp}$ is the component that
% is orthogonal. In this case

% $
% \alpha =\|\mathbf{y}-\mathbf{X} \mathbf{w}\|^{2} =\|\mathbf{y}_{\|}+\mathbf{y}_{\perp}-\mathbf{X} \mathbf{w}\|^{2} =\|\mathbf{y}_{\perp}\|^{2}+\|\mathbf{y}_{\|}-\mathbf{X} \mathbf{w}\|^{2} =\|\mathbf{y}_{\perp}\|^{2}+\|\mathbf{X}(\mathbf{w}_{0}-\mathbf{w})\|^{2} .
% $

% Hence this is then equivalent to the equation $\|\mathbf{X}(\mathbf{w}_{0}-\mathbf{w})\|^{2}=$ $\alpha-\|\mathbf{y}_{\perp}\|^{2}$, proving the claim. From this we also see that if $\mathbf{X}^{\top} \mathbf{X}$ is not full rank then what we get is not an ellipsoid but a cylinder with an ellipsoidal cross-subsection.


% \subsection*{Ridge Regression as MAP estimator}
% - Least squares as MLE:

% $
% \begin{aligned}
% & \mathbf{w}_{\mathrm{lse}} \stackrel{(a)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{y}, \mathbf{X} \mid \mathbf{w}) \\
% & \stackrel{(b)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{X} \mid \mathbf{w}) p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) \\
% & \stackrel{(c)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{X}) p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) \\
% & \stackrel{(d)}{=} \arg \min _{\mathbf{w}} \quad-\log p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) \\
% & \stackrel{(e)}{=} \arg \min _{\mathbf{w}} \quad-\log \left[\prod_{n=1}^{N} p\left(y_{n} \mid \mathbf{x}_{n}, \mathbf{w}\right)\right] \\
% & \stackrel{(f)}{=} \arg \min _{\mathbf{w}}-\log \left[\prod_{n=1}^{N} \mathcal{N}\left(y_{n} \mid \mathbf{x}_{n}^{\top} \mathbf{w}, \sigma^{2}\right)\right] \\
% & =\arg \min _{\mathbf{w}}-\log \left[\prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}}\right] \\
% & =\arg \min _{\mathbf{w}}-N \log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right)
% \\ & 
% \qquad +\sum_{n=1}^{N} \frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2} \\
% & =\arg \min _{\mathbf{w}} \frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}
% \end{aligned}
% $

% - Ridge as MAP:

% $
% \begin{aligned}
% & \mathbf{w}_{\text {ridge }}=\arg \min _{\mathbf{w}} \quad-\log p(\mathbf{w} \mid \mathbf{X}, \mathbf{y}) \\
% & \stackrel{(a)}{=} \arg \min _{\mathbf{w}}-\log \frac{p(\mathbf{y}, \mathbf{X} \mid \mathbf{w}) p(\mathbf{w})}{p(\mathbf{y}, \mathbf{X})} \\
% & \stackrel{(b)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{y}, \mathbf{X} \mid \mathbf{w}) p(\mathbf{w}) \\
% & \stackrel{(c)}{=} \arg \min _{\mathbf{w}} \quad-\log p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) p(\mathbf{w}) \\
% & =\arg \min _{\mathbf{w}}-\log \left[p(\mathbf{w}) \prod_{n=1}^{N} p\left(y_{n} \mid \mathbf{x}_{n}, \mathbf{w}\right)\right] \\
% & =\arg \min _{\mathbf{w}}-\log \mathcal{N}\left(\mathbf{w} \mid 0, \frac{1}{\lambda} \mathbf{I}\right) 
% \\ & \qquad \times \prod_{n=1}^{N} \mathcal{N}\left(y_{n} \mid \mathbf{x}_{n}^{\top} \mathbf{w}, \sigma^{2}\right) \\
% & =\arg \min _{\mathbf{w}}-\log \frac{1}{\left(2 \pi \frac{1}{\lambda}\right)^{D / 2}} e^{-\frac{\lambda}{2}\|\mathbf{w}\|^{2}} 
% \\ & \qquad \times
% \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}}
% \\
% & =\arg \min _{\mathbf{w}} \sum_{n=1}^{N} \frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}+\frac{\lambda}{2}\|\mathbf{w}\|^{2}
% \end{aligned}
% $

% - Ridge regression has a very similar interpretation. 

% - We start with the posterior $p(\mathbf{w} \mid \mathbf{X}, \mathbf{y})$ and chose that parameter $\mathbf{w}$ that maximizes this posterior. Hence this is called the maximum-a-posteriori (MAP) estimate. 

% - As before, we take the log and add a minus sign and minimize instead. 

% - In order to compute the posterior we use Bayes law and we assume that the components of the weight vector are iid Gaussians with mean zero and variance $\frac{1}{\lambda}$.

% In step (a) we used Bayes' law. In step (b) and (c) we eliminated quantities that do not depend on $\mathbf{w}$.
