\section*{Regularization}
$
\min _{\mathbf{w}} \mathcal{L}(\mathbf{w})+\Omega(\mathbf{w})
$

\subsection*{$L_{2}$-Regularization: Ridge Regression}
- Standard Euclidean norm $\left(L_{2^{-}}\right.$ norm):\\ $
\Omega(\mathbf{w})=\lambda\|\mathbf{w}\|_{2}^{2}
$ where $\|\mathbf{w}\|_{2}^{2}=\sum_{i} w_{i}^{2}$. 

- When $\mathcal{L}$ is MSE, this is called ridge regression:

$\min _{\mathbf{w}} \frac{1}{2 N} \sum_{n=1}^{N}\left[y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right]^{2}+\lambda\|\mathbf{w}\|_{2}^{2}$

- Least squares is a special case of this: set $\lambda:=0$.

\subsection*{Explicit solution for w:}

$\nabla = \nabla \mathcal{L} + \nabla\Omega = -\frac1NX^T(y-Xw) + \lambda2w = 0 \Rightarrow \mathbf{w}_{\text {ridge }}^{\star}=\left(\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I}\right)^{-1} \mathbf{X}^{\top} \mathbf{y}$

(here for simpler notation $\frac{\lambda^{\prime}}{2 N}=\lambda$ )

\subsection*{Ridge Regression Fights Ill-Conditioning}
- Lifting the eigenvalues : The eigenvalues of $\left(\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I}\right)$ are all at least $\lambda^{\prime}$ and so the inverse always exists.

- Proof: Write the Eigenvalue decomposition of $\mathbf{X}^{\top} \mathbf{X}$ as $\mathbf{U S U}^{\top}$. We then have

$
\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I} =\mathbf{U S} \mathbf{U}^{\top}+\lambda^{\prime} \mathbf{U I U}^{\top} =\mathbf{U}\left[\mathbf{S}+\lambda^{\prime} \mathbf{I}\right] \mathbf{U}^{\top}
$


- Alternative proof: Recall that for a symmetric matrix $\mathbf{A}$ we can also compute eigenvalues by looking at the so-called Rayleigh ratio,

$
R(\mathbf{A}, \mathbf{v})=\frac{\mathbf{v}^{\top} \mathbf{A} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}
$

Note that if $\mathbf{v}$ is an eigenvector with eigenvalue $\lambda$ then the Rayleigh coefficient indeed gives us $\lambda$. We can find the smallest and largest eigenvalue by minimizing and maximizing this coefficient. But note that if we apply this to the symmetric matrix $\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I}$ then for any vector $\mathbf{v}$ we have

$
\frac{\mathbf{v}^{\top}\left(\mathbf{X}^{\top} \mathbf{X}+\lambda^{\prime} \mathbf{I}\right) \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}} \geq \frac{\lambda^{\prime} \mathbf{v}^{\top} \mathbf{v}}{\mathbf{v}^{\top} \mathbf{v}}=\lambda^{\prime}
$

\subsection*{$L_{1}$-Regularization: The Lasso}
- $L_{1}$-norm in combination with the MSE cost function, this is known as the Lasso:

$\min _{\mathbf{w}} \frac{1}{2 N} \sum_{n=1}^{N}\left[y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right]^{2}+\lambda\|\mathbf{w}\|_{1}$

where $
\|\mathbf{w}\|_{1}:=\sum_{i}\left|w_{i}\right|
$

- For $L_{1}$-regularization the optimum solution is likely going to be sparse (only has few non-zero components) compared to the case where we use $L_{2}$-regularization. Sparsity is desirable, since it leads to a "simple" model.

\subsection*{Geometric Interpretation}

- Assume that $\mathbf{X}^{\top} \mathbf{X}$ is invertible. We claim that in this case the set $
\left\{\mathbf{w}:\|\mathbf{y}-\mathbf{X} \mathbf{w}\|^{2}=\alpha\right\}
$ is an ellipsoid and this ellipsoid simply scales around its origin as we change $\alpha$. 

- First look at $\alpha=\|\mathbf{X w}\|^{2}=\mathbf{w}^{\top} \mathbf{X}^{\top} \mathbf{X w}$. This is a quadratic form. Let $\mathbf{A}=\mathbf{X}^{\top} \mathbf{X}$. Note that $\mathbf{A}$ is a symmetric matrix and by assumption it has full rank. If $\mathbf{A}$ is a diagonal matrix with strictly positive elements $a_{i}$ along the diagonal then this describes the equation 
$
\sum_{i} a_{i} \mathbf{w}_{i}^{2}=\alpha
$ 
which is indeed the equation for an ellipsoid. In the general case, $\mathbf{A}$ can be written as (using the SVD) $\mathbf{A}=\mathbf{U B U}^{T}$, where $\mathbf{B}$ is a diagonal matrix with strictly positive entries. This then corresponds to an ellipsoid with rotated axes. If we now look at $\alpha=\|\mathbf{y}-\mathbf{X} \mathbf{w}\|^{2}$, where $\mathbf{y}$ is in the column space of $\mathbf{X}$ then we can write it as $\alpha=\left\|\mathbf{X}\left(\mathbf{w}_{0}-\mathbf{w}\right)\right\|^{2}$ for a suitable chosen $\mathbf{w}_{0}$ and so this corresponds to a shifted ellipsoid. Finally, for the general case, write $\mathbf{y}$ as $\mathbf{y}=\mathbf{y}_{\|}+$ $\mathbf{y}_{\perp}$, where $\mathbf{y}_{\|}$is the component of $\mathbf{y}$ that lies in the subspace spanned by the columns of $\mathbf{X}$ and $\mathbf{y}_{\perp}$ is the component that
is orthogonal. In this case

$$
\begin{aligned}
\alpha & =\|\mathbf{y}-\mathbf{X} \mathbf{w}\|^{2} \\
& =\left\|\mathbf{y}_{\|}+\mathbf{y}_{\perp}-\mathbf{X} \mathbf{w}\right\|^{2} \\
& =\left\|\mathbf{y}_{\perp}\right\|^{2}+\left\|\mathbf{y}_{\|}-\mathbf{X} \mathbf{w}\right\|^{2} \\
& =\left\|\mathbf{y}_{\perp}\right\|^{2}+\left\|\mathbf{X}\left(\mathbf{w}_{0}-\mathbf{w}\right)\right\|^{2} .
\end{aligned}
$$

Hence this is then equivalent to the equation $\left\|\mathbf{X}\left(\mathbf{w}_{0}-\mathbf{w}\right)\right\|^{2}=$ $\alpha-\left\|\mathbf{y}_{\perp}\right\|^{2}$, proving the claim. From this we also see that if $\mathbf{X}^{\top} \mathbf{X}$ is not full rank then what we get is not an ellipsoid but a cylinder with an ellipsoidal cross-subsection.


\subsection*{Ridge Regression as MAP estimator}
- Least squares as MLE:

$
\begin{aligned}
& \mathbf{w}_{\mathrm{lse}} \stackrel{(a)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{y}, \mathbf{X} \mid \mathbf{w}) \\
& \stackrel{(b)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{X} \mid \mathbf{w}) p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) \\
& \stackrel{(c)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{X}) p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) \\
& \stackrel{(d)}{=} \arg \min _{\mathbf{w}} \quad-\log p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) \\
& \stackrel{(e)}{=} \arg \min _{\mathbf{w}} \quad-\log \left[\prod_{n=1}^{N} p\left(y_{n} \mid \mathbf{x}_{n}, \mathbf{w}\right)\right] \\
& \stackrel{(f)}{=} \arg \min _{\mathbf{w}}-\log \left[\prod_{n=1}^{N} \mathcal{N}\left(y_{n} \mid \mathbf{x}_{n}^{\top} \mathbf{w}, \sigma^{2}\right)\right] \\
& =\arg \min _{\mathbf{w}}-\log \left[\prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}}\right] \\
& =\arg \min _{\mathbf{w}}-N \log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right)
\\ & 
\qquad +\sum_{n=1}^{N} \frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2} \\
& =\arg \min _{\mathbf{w}} \frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}
\end{aligned}
$

- Ridge as MAP:

$
\begin{aligned}
& \mathbf{w}_{\text {ridge }}=\arg \min _{\mathbf{w}} \quad-\log p(\mathbf{w} \mid \mathbf{X}, \mathbf{y}) \\
& \stackrel{(a)}{=} \arg \min _{\mathbf{w}}-\log \frac{p(\mathbf{y}, \mathbf{X} \mid \mathbf{w}) p(\mathbf{w})}{p(\mathbf{y}, \mathbf{X})} \\
& \stackrel{(b)}{=} \arg \min _{\mathbf{w}}-\log p(\mathbf{y}, \mathbf{X} \mid \mathbf{w}) p(\mathbf{w}) \\
& \stackrel{(c)}{=} \arg \min _{\mathbf{w}} \quad-\log p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) p(\mathbf{w}) \\
& =\arg \min _{\mathbf{w}}-\log \left[p(\mathbf{w}) \prod_{n=1}^{N} p\left(y_{n} \mid \mathbf{x}_{n}, \mathbf{w}\right)\right] \\
& =\arg \min _{\mathbf{w}}-\log \mathcal{N}\left(\mathbf{w} \mid 0, \frac{1}{\lambda} \mathbf{I}\right) 
\\ & \qquad \times \prod_{n=1}^{N} \mathcal{N}\left(y_{n} \mid \mathbf{x}_{n}^{\top} \mathbf{w}, \sigma^{2}\right) \\
& =\arg \min _{\mathbf{w}}-\log \frac{1}{\left(2 \pi \frac{1}{\lambda}\right)^{D / 2}} e^{-\frac{\lambda}{2}\|\mathbf{w}\|^{2}} 
\\ & \qquad \times
\prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}}
\\
& =\arg \min _{\mathbf{w}} \sum_{n=1}^{N} \frac{1}{2 \sigma^{2}}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2}+\frac{\lambda}{2}\|\mathbf{w}\|^{2}
\end{aligned}
$

- Ridge regression has a very similar interpretation. 

- We start with the posterior $p(\mathbf{w} \mid \mathbf{X}, \mathbf{y})$ and chose that parameter $\mathbf{w}$ that maximizes this posterior. Hence this is called the maximum-a-posteriori (MAP) estimate. 

- As before, we take the log and add a minus sign and minimize instead. 

- In order to compute the posterior we use Bayes law and we assume that the components of the weight vector are iid Gaussians with mean zero and variance $\frac{1}{\lambda}$.

In step (a) we used Bayes' law. In step (b) and (c) we eliminated quantities that do not depend on $\mathbf{w}$.
