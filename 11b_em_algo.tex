
\section*{Expectation-Maximization Algorithm}
- Computing maximum likelihood for Gaussian mixture model is difficult due to the log outside the sum.

$\max _{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}):=\sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)$

- EM algorithm uses an iterative two-step procedure where individual steps usually involve problems that are easy to optimize.

\section*{EM algorithm:}
Start with $\boldsymbol{\theta}^{(1)}$ and iterate:

\begin{enumerate}
  \item Expectation step: Compute a lower bound to the cost such that it is tight at the previous $\boldsymbol{\theta}^{(t)}$ :
\end{enumerate}

$\mathcal{L}(\boldsymbol{\theta}) \geq \underline{\mathcal{L}}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{(t)}\right)$ and
$\mathcal{L}\left(\boldsymbol{\theta}^{(t)}\right)=\underline{\mathcal{L}}\left(\boldsymbol{\theta}^{(t)}, \boldsymbol{\theta}^{(t)}\right)$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Maximization step: Update $\boldsymbol{\theta}$ :
\end{enumerate}

$
\boldsymbol{\theta}^{(t+1)}=\arg \max _{\boldsymbol{\theta}} \underline{\mathcal{L}}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{(t)}\right) .
$

\section*{Jensen's Inequatlity}
- Log is convex $\rightarrow$ Concavity of log

- Given non-negative weights $q$ s.t. $\sum_{k} q_{k}=1$, the following holds for any $r_{k}>0$ :

$\log \left(\sum_{k=1}^{K} q_{k} r_{k}\right) \geq \sum_{k=1}^{K} q_{k} \log r_{k}$

\section*{The expectation step}
$\log \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right) \geq \sum_{k=1}^{K} q_{k n} \log \frac{\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{q_{k n}}$
with equality when,
$q_{k n}=\frac{\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}$

\section*{The maximization step}
- Maximize the lower bound w.r.t. $\boldsymbol{\theta}$.

$\max _{\boldsymbol{\theta}} \sum_{n=1}^{N} \sum_{k=1}^{K} q_{k n}^{(t)}\left[\log \pi_{k}+\log \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right]$

- Differentiating w.r.t. $\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}^{-1}$, we can get the updates for $\boldsymbol{\mu}_{k}$ and $\boldsymbol{\Sigma}_{k}$.

% $
% \begin{aligned}
% \boldsymbol{\mu}_{k}^{(t+1)} & :=\frac{\sum_{n} q_{k n}^{(t)} \mathbf{x}_{n}}{\sum_{n} q_{k n}^{(t)}} \\
% \boldsymbol{\Sigma}_{k}^{(t+1)} & :=\frac{\sum_{n} q_{k n}^{(t)}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{(t+1)}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{(t+1)}\right)^{\top}}{\sum_{n} q_{k n}^{(t)}}
% \end{aligned}
% $
$\boldsymbol{\mu}_{k}^{(t+1)} := \frac{\sum_{n} q_{k n}^{(t)} \mathbf{x}_{n}}{\sum_{n} q_{k n}^{(t)}} 
\\ \boldsymbol{\Sigma}_{k}^{(t+1)} := \frac{\sum_{n} q_{k n}^{(t)} (\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{(t+1)})(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{(t+1)})^{\top}}{\sum_{n} q_{k n}^{(t)}}$


For $\pi_{k}$, we use the fact that they sum to 1. Therefore, we add a Lagrangian term, differentiate w.r.t. $\pi_{k}$ and set to 0 , to get the following update:

$
\pi_{k}^{(t+1)}:=\frac{1}{N} \sum_{n=1}^{N} q_{k n}^{(t)}
$

\section*{EM for GMM}
Initialize $\boldsymbol{\mu}^{(1)}, \boldsymbol{\Sigma}^{(1)}, \boldsymbol{\pi}^{(1)}$ and iterate between the $\mathrm{E}$ and $\mathrm{M}$ step, until $\mathcal{L}(\boldsymbol{\theta})$ stabilizes.

\begin{enumerate}
  \item E-step: Compute assignments $q_{k n}^{(t)}$ :
\end{enumerate}

$
q_{k n}^{(t)}:=\frac{\pi_{k}^{(t)} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}^{(t)}, \boldsymbol{\Sigma}_{k}^{(t)}\right)}{\sum_{k=1}^{K} \pi_{k}^{(t)} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}^{(t)}, \boldsymbol{\Sigma}_{k}^{(t)}\right)}
$

\begin{enumerate}
  \setcounter{enumi}{1}
  \item Compute the marginal likelihood (cost).
\end{enumerate}

$
\mathcal{L}\left(\boldsymbol{\theta}^{(t)}\right)=\sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_{k}^{(t)} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}^{(t)}, \boldsymbol{\Sigma}_{k}^{(t)}\right)
$

\begin{enumerate}
  \setcounter{enumi}{2}
  \item M-step: Update $\boldsymbol{\mu}_{k}^{(t+1)}, \boldsymbol{\Sigma}_{k}^{(t+1)}, \pi_{k}^{(t+1)}$.
\end{enumerate}

$
\begin{aligned}
\boldsymbol{\mu}_{k}^{(t+1)} & :=\frac{\sum_{n} q_{k n}^{(t)} \mathbf{x}_{n}}{\sum_{n} q_{k n}^{(t)}} \\
\boldsymbol{\Sigma}_{k}^{(t+1)} & :=\frac{\sum_{n} q_{k n}^{(t)}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{(t+1)}\right)\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{(t+1)}\right)^{\top}}{\sum_{n} q_{k n}^{(t)}} \\
\pi_{k}^{(t+1)} & :=\frac{1}{N} \sum_{n} q_{k n}^{(t)}
\end{aligned}
$

- If we let the covariance be diagonal i.e. $\boldsymbol{\Sigma}_{k}:=\sigma^{2} \mathbf{I}$, then EM algorithm is same as K-means as $\sigma^{2} \rightarrow 0$.

\section*{Posterior distribution}
- show that $q_{k n}^{(t)}$ is the posterior distribution of the latent variable, i.e. $q_{k n}^{(t)}=p\left(z_{n}=k \mid \mathbf{x}_{n}, \boldsymbol{\theta}^{(t)}\right)$

$p\left(\mathbf{x}_{n}, z_{n} \mid \boldsymbol{\theta}\right)=p\left(\mathbf{x}_{n} \mid z_{n}, \boldsymbol{\theta}\right) p\left(z_{n} \mid \boldsymbol{\theta}\right)=p\left(z_{n} \mid \mathbf{x}_{n}, \boldsymbol{\theta}\right) p\left(\mathbf{x}_{n} \mid \boldsymbol{\theta}\right)$

$\underbrace{p(A,B)}_{\text{joint}}=\underbrace{p(A|B)}_{\text{likelyhood}}\underbrace{p(B)}_{\text{prior}}=\underbrace{p(B|A)}_{\text{posterior}}\underbrace{p(A)}_{\text{marginal}}$

\section*{EM in general}
- Given a general joint distribution $p\left(\mathbf{x}_{n}, z_{n} \mid \boldsymbol{\theta}\right)$, the marginal likelihood can be lower bounded similarly. Compact EM:
$\boldsymbol{\theta}^{(t+1)} \\:= \arg \max_{\boldsymbol{\theta}} \sum_{n=1}^{N} \mathbb{E}_{p(z_{n} \mid \mathbf{x}_{n}, \boldsymbol{\theta}^{(t)})}[\log p(\mathbf{x}_{n}, z_{n} \mid \boldsymbol{\theta})]$

- Another interpretation is that part of the data is missing, i.e. $\left(\mathbf{x}_{n}, z_{n}\right)$ is the "complete" data and $z_{n}$ is missing. The EM algorithm averages over the "unobserved" part of the data.