\section*{Optimisation}

- Given $\mathcal{L}(\mathbf{w})$ we want $\mathbf{w^*} \in \mathbb{R}^D$ which minimises the cost: $\min_\mathbf{w} \mathcal{L}(\mathbf{w}) \rightarrow$ formulated as an optimisation problem

- Local minimum $\mathbf{w^*} \Rightarrow \exists \epsilon > 0$ s.t. \\
$\mathcal{L}(\mathbf{w^*}) \leq \mathcal{L}(\mathbf{w}) \ \forall \mathbf{w} \ \mathrm{with} \ \Vert \mathbf{w}-\mathbf{w^*} \Vert < \epsilon$

- Global minimum $\mathbf{w^*}$,
$\mathcal{L}(\mathbf{w^*}) \leq \mathcal{L}(\mathbf{w}) \ \forall \mathbf{w} \in \mathbb{R}^D$


\subsection*{Smooth Optimization}
- A gradient is the slope of the tangent to the function. It points to the direction of largest increase of the function.

$\nabla \mathcal{L}(\mathbf{w}):=\left[\frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_{1}}, \ldots, \frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_{D}}\right]^{\top} \in \mathbb{R}^{D}$

\subsection*{Gradient Descent}
- To minimize the function, we iteratively take a step in the opposite direction of the gradient

$$
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \nabla \mathcal{L}\left(\mathbf{w}^{(t)}\right)
$$

where $\gamma>0$ is the step-size (or learning rate). Then repeat with the next $t$.

- Example: Gradient descent for 1parameter model to minimize MSE:

$f_w(x)=w_0\Rightarrow\mathcal{L}(w)=\frac1{2N}\sum_{n=1}^N\left(y_n-w_0\right)^2=\nabla \mathcal{L}=\frac\partial{\partial w_0}\mathcal{L}=\frac{1}{2N}\sum-2(y_{n}-w_{0})=(-\frac{1}{N}\sum y_{n})+w_{0}=w_0-\bar{y}$

($min_{w}L(w)=w_{0}-\bar{y}=0\Rightarrow w_{0}=\bar{y}$)

$
w_{0}^{(t+1)}:=(1-\gamma) w_{0}^{(t)}+\gamma \bar{y}
$

where $\bar{y}:=\sum_{n} y_{n} / N$. When is this sequence guaranteed to converge? When $\gamma>2$ you start having an exploding GD.

\subsection*{Gradient Descent for Linear MSE}
% For linear regression

% $\mathbf{y}=\left[\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{N}\end{array}\right], \mathbf{X}=\left[\begin{array}{cccc}x_{11} & x_{12} & \ldots & x_{1 D} \\ x_{21} & x_{22} & \ldots & x_{2 D} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N 1} & x_{N 2} & \ldots & x_{N D}\end{array}\right]$

We define the error vector $\mathbf{e}$ :
$\mathrm{e}=\mathbf{y}-\mathbf{X w}$

and MSE as follows:

$
\mathcal{L}(\mathbf{w}) :=\frac{1}{2 N} \sum_{n=1}^{N}\left(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w}\right)^{2} =\frac{1}{2 N} \mathbf{e}^{\top} \mathbf{e}
$

then the gradient is given by

$\nabla \mathcal{L}(\mathbf{w})=-\frac{1}{N} \mathbf{X}^{\top} \mathbf{e}$

Computational cost: $\Theta(N\times D)$

% \subsection*{Variant with offset. Recall: Alter-}
% native trick when also incorporating an offset term for the regression:

% $\mathbf{y}=\left[\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{N}\end{array}\right] \quad \widetilde{\mathbf{X}}=\left[\begin{array}{ccccc}1 & x_{11} & x_{12} & \ldots & x_{1 D} \\ 1 & x_{21} & x_{22} & \ldots & x_{2 D} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{N 1} & x_{N 2} & \ldots & x_{N D}\end{array}\right]$

\subsection*{Stochastic Gradient Descent}

$
\mathcal{L}(\mathbf{w})=\frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_{n}(\mathbf{w})
$

$
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \nabla \mathcal{L}_{n}\left(\mathbf{w}^{(t)}\right)
$

- Compuational cost: $\Theta(D)$

- Cheap and unbiased estimate of the gradient!

$E[\nabla \mathcal{L}_n(w)]=\frac1n\sum_{n=1}^N\nabla \mathcal{L}_n(w)=\nabla\left(\frac1N\sum\cdots\right)=\nabla \mathcal{L}(n)$
which is the true gradient direction. 

\subsection*{Mini-batch SGD}

$
\mathbf{g}:=\frac{1}{|B|} \sum_{n \in B} \nabla \mathcal{L}_{n}\left(\mathbf{w}^{(t)}\right)
$

$
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \mathbf{g} .
$

- Randomly chosen a subset $B \subseteq[N]$ of the training examples. For each of these selected examples $n$, we compute the respective gradient $\nabla \mathcal{L}_{n}$, at the same current point $\mathbf{w}^{(t)}$.

- The computation of $\mathbf{g}$ can be parallelized easily. This is how current deep-learning applications utilize GPUs (by running over $|B|$ threads in parallel).

- $B:=$ $[N]$, we obtain $\mathbf{g}=\nabla \mathcal{L}$.

\subsection*{Non-Smooth Optimization}
- An alternative characterization of convexity, for differentiable functions is given by

$\mathcal{L}(\mathbf{u}) \geq \mathcal{L}(\mathbf{w})+\nabla \mathcal{L}(\mathbf{w})^{\top}(\mathbf{u}-\mathbf{w}) \quad \forall \mathbf{u}, \mathbf{w}$

meaning that the function must always lie above its linearization.

\subsection*{Subgradients}
- A vector $\mathbf{g} \in \mathbb{R}^{D}$ such that

$
\mathcal{L}(\mathbf{u}) \geq \mathcal{L}(\mathbf{w})+\mathbf{g}^{\top}(\mathbf{u}-\mathbf{w}) \quad \forall \mathbf{u}
$

is called a subgradient to the function $\mathcal{L}$ at $\mathbf{w}$.

- This definition makes sense for objectives $\mathcal{L}$ which are not necessarily differentiable (and not even necessarily convex).

- If $\mathcal{L}$ is convex and differentiable at $\mathbf{w}$, then the only subgradient at $\mathbf{w}$ is $\mathbf{g}=\nabla \mathcal{L}(\mathbf{w})$.

\subsection*{Subgradient Descent}
$
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \mathbf{g}
$

for $\mathbf{g}$ a subgradient to $\mathcal{L}$ at the current iterate $\mathbf{w}^{(t)}$.

\subsection*{Example: Optimizing Linear MAE}
1) Compute a subgradient of the absolute value function
$h: \mathbb{R} \rightarrow \mathbb{R}, h(e):=|e|$.

$g=\left\{\begin{array}{cc}-1 & \text { if } e<0 \\ {[-1,1]} & \text { if } e=0 \\ 1 & \text { if } e>0\end{array}\right.$

2) Recall the definition of the mean absolute error:

$\mathcal{L}(\mathbf{w})=\operatorname{MAE}(\mathbf{w}):=\frac{1}{N} \sum_{n=1}^{N}\left|y_{n}-f_{\mathbf{w}}\left(\mathbf{x}_{n}\right)\right|$

For linear regression, its (sub)gradient is easy to compute using the chain rule. Compute it!

The subgradient is given by:

$
\partial\mathcal{L}(\mathbf{w})=\frac1N\sum_{n=1}^Ng(e_n)\nabla f_\mathbf{w}(\mathbf{x}_n) = \frac{1}{N} \sum_{n=1}^{N} g(e_n) \mathbf{x}_n
$

since $f_\mathbf{w}(\mathbf{x}_n)=\mathbf{w}^T\mathbf{x}_n,\text{then }\nabla f_\mathbf{w}(\mathbf{x}_n)=\mathbf{x}_n$
where \(e_n = y_n - f_{\mathbf{w}}(\mathbf{x}_n)\).

See Exercise Sheet 2.

\subsection*{Stochastic Subgradient Descent}
- still abbreviated SGD commonly.

- Same, $\mathbf{g}$ being a subgradient to the randomly selected $\mathcal{L}_{n}$ at the current iterate $\mathbf{w}^{(t)}$.

- SGD update for linear MAE: \\ $\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}+\gamma g(e_n)\mathbf{x}_n$

\subsection*{Variants of SGD}
\subsubsection*{SGD with Momentum}
pick a stochastic gradient $\mathbf{g}$

$
\begin{aligned}
& \mathbf{m}^{(t+1)}:=\beta_{1} \mathbf{m}^{(t)}+\left(1-\beta_{1}\right) \mathbf{g} \quad \text { (momentum term) } \\
& \mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \mathbf{m}^{(t+1)}
\end{aligned}
$

- momentum from previous gradients (acceleration)

\subsubsection*{Adam}
- pick a stochastic gradient $\mathbf{g}$

$
\mathbf{m}^{(t+1)} := \beta_{1} \mathbf{m}^{(t)} + \left(1-\beta_{1}\right) \mathbf{g} \\
\mathbf{v}_{i}^{(t+1)} := \beta_{2} \mathbf{v}_{i}^{(t)} + \left(1-\beta_{2}\right)\left(\mathbf{g}_{i}\right)^{2} \quad \forall i \quad \text{(Momentum term)} \\
\mathbf{w}_{i}^{(t+1)} := \mathbf{w}_{i}^{(t)} - \frac{\gamma}{\sqrt{\mathbf{v}_{i}^{(t+1)}}} \mathbf{m}_{i}^{(t+1)} \quad \forall i
$

- faster forgetting of older weights

- is a momentum variant of Adagrad

- coordinate-wise adjusted learning rate

-strong performance in practice, e.g. for self-attention networks


\subsubsection*{SignSGD}
pick a stochastic gradient $\mathbf{g}$

$
\mathbf{w}_{i}^{(t+1)}:=\mathbf{w}_{i}^{(t)}-\gamma \operatorname{sign}\left(\mathbf{g}_{i}\right)
$

- only use the sign (one bit) of each gradient entry $\rightarrow$ communication efficient for distributed training

- convergence issues

\subsection*{Constrained Optimization}
- Sometimes, optimization problems come posed with additional constraints:

$\min _{\mathbf{w}} \mathcal{L}(\mathbf{w}), \quad$ subject to $\mathbf{w} \in \mathcal{C}$.

- The set $\mathcal{C} \subset \mathbb{R}^{D}$ is called the constraint set.

\subsection*{Convex Sets}
A set $\mathcal{C}$ is convex iff the line segment between any two points of $\mathcal{C}$ lies in $\mathcal{C}$, i.e., if for any $\mathbf{u}, \mathbf{v} \in \mathcal{C}$ and any $\theta$ with $0 \leq \theta \leq 1$, we have

$
\theta \mathbf{u}+(1-\theta) \mathbf{v} \in \mathcal{C}
$

- Intersubsections of convex sets are convex
- Projections onto convex sets are unique.(and often efficient to compute)

- Formal definition:

$P_{\mathcal{C}}\left(\mathbf{w}^{\prime}\right):=\arg \min _{\mathbf{v} \in \mathcal{C}}\left\|\mathbf{v}-\mathbf{w}^{\prime}\right\|$.

\subsection*{Projected Gradient Descent}

\begin{wrapfigure}{r}{0.3\columnwidth} 
    \centering
    \includegraphics*[width=0.3\columnwidth]{figures/PGD.jpg}
    \vspace{-10pt}
\end{wrapfigure}

- Idea: add a projection onto $\mathcal{C}$ after every step:

$
P_{\mathcal{C}}\left(\mathbf{w}^{\prime}\right):=\arg \min _{\mathbf{v} \in \mathcal{C}}\left\|\mathbf{v}-\mathbf{w}^{\prime}\right\| .
$

$
\mathbf{w}^{(t+1)}:=P_{\mathcal{C}}\left[\mathbf{w}^{(t)}-\gamma \nabla \mathcal{L}\left(\mathbf{w}^{(t)}\right)\right]
$

- Projected SGD. Same SGD step, followed by the projection step. Same convergence properties.

\subsection*{Constrained $\rightarrow$ Unconstrained Problems}

$\min_{w\in C}\mathcal{L}(w)\sim\min_{w}\mathcal{L}(w)+P(w)$

- Alternatives to projected gradient methods

- Use penalty functions instead of directly solving $\min _{\mathbf{w} \in \mathcal{C}} \mathcal{L}(\mathbf{w})$.

- ``brick wall" (indicator function)

$
P(w) = I_{\mathcal{C}}(\mathbf{w}):= \begin{cases}0 & \mathbf{w} \in \mathcal{C} \\
\infty & \mathbf{w} \notin \mathcal{C}\end{cases}
$

Can't run GD because non-continuous objective.

- Penalize error. Example:

$
\mathcal{C}=\left\{\mathbf{w} \in \mathbb{R}^{D} \mid A \mathbf{w}=\mathbf{b}\right\} \Rightarrow P(w) = \lambda\|A \mathbf{w}-\mathbf{b}\|^{2}
$

- $\ell_1$ norm encourages sparsity which reduces memory:
$
P(w) = \lambda\|\mathbf{w}\|_1
$

- Linearized Penalty Functions (see Lagrange Multipliers)

\subsection*{Implementation Issues}

1) Stopping criteria: $\nabla \mathcal{L}(\mathbf{w}) \approx 0$

2) Optimality: 

\underline{1st order optimiality condition}: 
if $\mathcal{L} \text{ is convex and } \nabla \mathcal{L}(w^*)=0 \Rightarrow$ global optimality

\underline{2nd order}:
$\mathcal{L} \text{ potentially non-convex, if } \nabla \mathcal{L}(w^*)=0 \ \& \ \nabla^2 \mathcal{L}(w^*) \geq 0 \Rightarrow w$ local minimum.

$\nabla^{2} \mathcal{L}(\mathbf{w}):=\frac{\partial^{2} \mathcal{L}}{\partial \mathbf{w} \partial \mathbf{w}^{\top}}(\mathbf{w})$ is expensive.

3) Step-size selection: $\gamma >>$ might diverge. $\gamma <<$ slow convergence. Convergence to local minimum guaranteed only when $\gamma<\gamma_{\text {min }}$, $\gamma_{\min }$ depends on the problem.

4) Line-search methods: For some $\mathcal{L}$, set step-size automatically.

5) Feature normalization: 
GD is sensitive to ill-conditioning $\rightarrow$ Normalize your input features i.e. pre-condition the optimization problem. 

\subsection*{Non-Convex Optimization}

- Real-world problems are not convex!

- All we have learnt on algorithm design and performance of convex algorithms still helps us in the nonconvex world.

\subsection*{SGD Theory}
- For convergence, $\gamma^{(t)} \rightarrow 0$ ``appropriately". Robbins-Monroe condition suggests to take $\gamma^{(t)}$ s.t.:

$
\sum_{t=1}^{\infty} \gamma^{(t)}=\infty, \quad \sum_{t=1}^{\infty}\left(\gamma^{(t)}\right)^{2}<\infty
$

Example: $\gamma^{(t)}:=1 /(t+1)^{r}$ where $r \in(0.5,1)$.