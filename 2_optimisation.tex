\section{Optimisation}

% - Given $\mathcal{L}(\mathbf{w})$ we want $\mathbf{w^*} \in \mathbb{R}^D$ which minimises the cost: $\min_\mathbf{w} \mathcal{L}(\mathbf{w})\rightarrow$ formulated as an optimisation problem

- Local min $\mathbf{w^*}\rightarrow \exists \epsilon > 0$ s.t.
$\mathcal{L}(\mathbf{w^*}) \leq \mathcal{L}(\mathbf{w}) \ \forall \mathbf{w} \ \text{w/} \ \Vert \mathbf{w}-\mathbf{w^*} \Vert < \epsilon$

- Global minimum $\mathbf{w^*}$,
$\mathcal{L}(\mathbf{w^*}) \leq \mathcal{L}(\mathbf{w}) \ \forall \mathbf{w} \in \mathbb{R}^D$


% \subsection*{Smooth Optimization}
% - A gradient is the slope of the tangent to the function. It points to the direction of largest increase of the function.

% $\nabla \mathcal{L}(\mathbf{w}):=[\frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_{1}}, \ldots, \frac{\partial \mathcal{L}(\mathbf{w})}{\partial w_{D}}]^{\top} \in \mathbb{R}^{D}$

\subsection*{Gradient Descent (GD): $
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \nabla \mathcal{L}(\mathbf{w}^{(t)}), \gamma>0
$}
% - To minimize the function, we iteratively take a step in the opposite direction of the gradient


% , $\gamma>0$ step-size/learning rate 
% Then repeat with the next $t$.

% - Example: Gradient descent for 1parameter model to minimize MSE:

% $f_w(x)=w_0arrow\mathcal{L}(w)=\frac1{2N}\sum_{n=1}^N(y_n-w_0)^2=\nabla \mathcal{L}=\frac\partial{\partial w_0}\mathcal{L}=\frac{1}{2N}\sum-2(y_{n}-w_{0})=(-\frac{1}{N}\sum y_{n})+w_{0}=w_0-\bar{y}$

% ($min_{w}L(w)=w_{0}-\bar{y}=0arrow w_{0}=\bar{y}$)

% $
% w_{0}^{(t+1)}:=(1-\gamma) w_{0}^{(t)}+\gamma \bar{y}
% $

% where $\bar{y}:=\sum_{n} y_{n} / N$. When is this sequence guaranteed to converge? When $\gamma>2$ you start having an exploding GD.

\subsection*{Gradient Descent for Linear MSE, $\mathcal{O}(N\times D)$}
% For linear regression

% $\mathbf{y}=[\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{N}\end{array}], \mathbf{X}=[\begin{array}{cccc}x_{11} x_{12} \ldots x_{1 D} \\ x_{21} x_{22} \ldots x_{2 D} \\ \vdots \vdots \ddots \vdots \\ x_{N 1} x_{N 2} \ldots x_{N D}\end{array}]$


$
\mathcal{L}(\mathbf{w}) :=\frac{1}{2 N} \sum_{n=1}^{N}(y_{n}-\mathbf{x}_{n}^{\top} \mathbf{w})^{2} =\frac{1}{2 N} \mathbf{e}^{\top} \mathbf{e}
$,
$\nabla \mathcal{L}(\mathbf{w})=-\frac{1}{N} \mathbf{X}^{\top} \mathbf{e}$

$\mathbf{e} := \mathbf{y}-\mathbf{X w}$

% \subsection*{Variant with offset. Recall: Alter-}
% native trick when also incorporating an offset term for the regression:

% $\mathbf{y}=[\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{N}\end{array}] \quad \widetilde{\mathbf{X}}=[\begin{array}{ccccc}1 x_{11} x_{12} \ldots x_{1 D} \\ 1 x_{21} x_{22} \ldots x_{2 D} \\ \vdots \vdots \vdots \ddots \vdots \\ 1 x_{N 1} x_{N 2} \ldots x_{N D}\end{array}]$

\subsection*{Stochastic Gradient Descent (SGD), $\mathcal{O}(D)$}

$
\mathcal{L}(\mathbf{w})=\frac{1}{N} \sum_{n=1}^{N} \mathcal{L}_{n}(\mathbf{w})
$
,
$
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \nabla \mathcal{L}_{n}(\mathbf{w}^{(t)})
$

- Cheap and unbiased estimate of the gradient
$E[\nabla \mathcal{L}_n(w)]=\frac1n\sum_{n=1}^N\nabla \mathcal{L}_n(w)=\nabla(\frac1N\sum\cdots)=\nabla \mathcal{L}(n)$
% which is the true gradient direction. 

\subsection*{Mini-batch SGD: $
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \mathbf{g}
$}

$
\mathbf{g}:=\frac{1}{|B|} \sum_{n \in B} \nabla \mathcal{L}_{n}(\mathbf{w}^{(t)})
$, subset $B \subseteq[N]$ of training samples

% - The computation of $\mathbf{g}$ can be parallelized easily. This is how current deep-learning applications utilize GPUs (by running over $|B|$ threads in parallel).

% - $B:=$ $[N]$, we obtain $\mathbf{g}=\nabla \mathcal{L}$.

\subsection*{Non-Smooth Optimization}
- Convexity, for differentiable functions:
$\mathcal{L}(\mathbf{u}) \geq \mathcal{L}(\mathbf{w})+\nabla \mathcal{L}(\mathbf{w})^{\top}(\mathbf{u}-\mathbf{w}) \quad \forall \mathbf{u}, \mathbf{w}$ 
% i.e. the function must always lie above its linearization.

\subsection*{Subgradients}
- A vector $\mathbf{g} \in \mathbb{R}^{D}$ s.t.
$
\mathcal{L}(\mathbf{u}) \geq \mathcal{L}(\mathbf{w})+\mathbf{g}^{\top}(\mathbf{u}-\mathbf{w}) \quad \forall \mathbf{u}
$
is called a subgradient to the function $\mathcal{L}$ at $\mathbf{w}$.

- This definition makes sense for objectives $\mathcal{L}$ which are not necessarily differentiable (and not even necessarily convex).

- $\mathcal{L}$ convex and differentiable at $\mathbf{w}\rightarrow$ only subgradient $\mathbf{g}=\nabla \mathcal{L}(\mathbf{w})$.

\subsection*{Subgradient Descent}
$
\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \mathbf{g}
$
for $\mathbf{g}$ a subgradient to $\mathcal{L}$

% \subsection*{Example: Optimizing Linear MAE}
% 1) $h: \mathbb{R}\rightarrow \mathbb{R}, h(e):=|e|$,
% $g=\{\begin{array}{cc}-1 \text { if } e<0 \\ {[-1,1]} \text { if } e=0 \\ 1 \text { if } e>0\end{array}.$

% % 2) Recall the definition of the mean absolute error:

% % $\mathcal{L}(\mathbf{w})=\operatorname{MAE}(\mathbf{w}):=\frac{1}{N} \sum_{n=1}^{N}|y_{n}-f_{\mathbf{w}}(\mathbf{x}_{n})|$

% 2) The subgradient is given by:
% $
% \partial\mathcal{L}(\mathbf{w})=\frac1N\sum_{n=1}^Ng(e_n)\nabla f_\mathbf{w}(\mathbf{x}_n) = \frac{1}{N} \sum_{n=1}^{N} g(e_n) \mathbf{x}_n
% $
% since $f_\mathbf{w}(\mathbf{x}_n)=\mathbf{w}^T\mathbf{x}_n$


% \subsection*{Stochastic Subgradient Descent}
% % - still abbreviated SGD commonly.

% % - Same, $\mathbf{g}$ being a subgradient to the randomly selected $\mathcal{L}_{n}$ at the current iterate $\mathbf{w}^{(t)}$.

% - SGD update for linear MAE: $\mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}+\gamma g(e_n)\mathbf{x}_n$

% \subsection*{Variants of SGD with a stochastic gradient $\mathbf{g}$:}

% \subsubsection*{SGD with Momentum}

% $
% \mathbf{m}^{(t+1)}:=\beta_{1} \mathbf{m}^{(t)}+(1-\beta_{1}) \mathbf{g} \quad \text { (momentum term) } \\
% \mathbf{w}^{(t+1)}:=\mathbf{w}^{(t)}-\gamma \mathbf{m}^{(t+1)}
% $

% - momentum from previous gradients (acceleration)

% \subsubsection*{Adam}


% $
% \mathbf{m}^{(t+1)} := \beta_{1} \mathbf{m}^{(t)} + (1-\beta_{1}) \mathbf{g} \\
% \mathbf{v}_{i}^{(t+1)} := \beta_{2} \mathbf{v}_{i}^{(t)} + (1-\beta_{2})(\mathbf{g}_{i})^{2} \quad \forall i \quad \text{(Momentum term)} \\
% \mathbf{w}_{i}^{(t+1)} := \mathbf{w}_{i}^{(t)} - \frac{\gamma}{\sqrt{\mathbf{v}_{i}^{(t+1)}}} \mathbf{m}_{i}^{(t+1)} \quad \forall i
% $

% - faster forgetting of older weights

% % - is a momentum variant of Adagrad

% - coordinate-wise adjusted learning rate

% % -strong performance in practice, e.g. for self-attention networks


% \subsubsection*{SignSGD}


% $
% \mathbf{w}_{i}^{(t+1)}:=\mathbf{w}_{i}^{(t)}-\gamma \operatorname{sign}(\mathbf{g}_{i})
% $

% - only use the sign (one bit) of each gradient entry $\rightarrow$ communication efficient for distributed training, but convergence issues

% \subsection*{Constrained Optimization}
% % - Sometimes, optimization problems come posed with additional constraints:

% $\min _{\mathbf{w}} \mathcal{L}(\mathbf{w}), \quad$ subject to $\mathbf{w} \in \mathcal{C}$, $\subset \mathbb{R}^{D} \rightarrow$ constraint set

\subsection*{Convex Sets}
% A set $\mathcal{C}$ is convex iff the line segment between any two points of $\mathcal{C}$ lies in $\mathcal{C}$, i.e., if for any $\mathbf{u}, \mathbf{v} \in \mathcal{C}$ and any $\theta$ with $0 \leq \theta \leq 1$, we have
% $
% \theta \mathbf{u}+(1-\theta) \mathbf{v} \in \mathcal{C}
% $

- Intersubsections of convex sets are convex

- Projections onto convex sets are unique.

- Formal definition:
$P_{\mathcal{C}}(\mathbf{w}^{\prime}):=\arg \min _{\mathbf{v} \in \mathcal{C}}\|\mathbf{v}-\mathbf{w}^{\prime}\|$.

\subsection*{Projected Gradient Descent: SGD + Projection step}

% \begin{wrapfigure}{r}{0.3\columnwidth} 
%     \centering
%     \includegraphics*[width=0.3\columnwidth]{figures/PGD.jpg}
%     \vspace{-10pt}
% \end{wrapfigure}

- Idea: add a projection onto $\mathcal{C}$ after every step:

$
P_{\mathcal{C}}(\mathbf{w}^{\prime}):=\arg \min _{\mathbf{v} \in \mathcal{C}}\|\mathbf{v}-\mathbf{w}^{\prime}\|
$, 
$
\mathbf{w}^{(t+1)}:=P_{\mathcal{C}}[\mathbf{w}^{(t)}-\gamma \nabla \mathcal{L}(\mathbf{w}^{(t)})]
$

% - Projected SGD. Same SGD step, followed by the projection step. Same convergence properties.

\subsection*{Constrained $\rightarrow$ Unconstrained Problems}

$\min_{w\in C}\mathcal{L}(w)\sim\min_{w}\mathcal{L}(w)+P(w)$

% - Alternatives to projected gradient methods

% - Penalty functions instead of $\min _{\mathbf{w} \in \mathcal{C}} \mathcal{L}(\mathbf{w})$.

% - ``brick wall" (indicator function): 
% $
% P(w) = I_{\mathcal{C}}(\mathbf{w}):= \begin{cases}0 \mathbf{w} \in \mathcal{C} \\
% \infty \mathbf{w} \notin \mathcal{C}\end{cases}
% $

% Can't run GD because non-continuous objective.

% - Penalize error, e.g.:
% $
% \mathcal{C}=\{\mathbf{w} \in \mathbb{R}^{D} \mid A \mathbf{w}=\mathbf{b}\}\rightarrow P(w) = \lambda\|A \mathbf{w}-\mathbf{b}\|^{2}
% $

% - $\ell_1$ norm $\uparrow$ sparsity, $\downarrow$ memory:
% $
% P(w) = \lambda\|\mathbf{w}\|_1
% $

% - Linearized Penalty Functions (see Lagrange Multipliers)

\subsection*{Implementation Issues}

% 1) Stopping criteria: $\nabla \mathcal{L}(\mathbf{w}) \approx 0$

Optimality: 
\underline{1st order}: 
if $\mathcal{L} \text{ is cvx and } \nabla \mathcal{L}(w^*)=0\rightarrow$ global

\underline{2nd order}:
$\mathcal{L} \text{ potentially non-convex, if } \nabla \mathcal{L}(w^*)=0 \ , \nabla^2 \mathcal{L}(w^*) \geq 0\rightarrow w$ local minimum ($\nabla^{2} \mathcal{L}(\mathbf{w}):=\frac{\partial^{2} \mathcal{L}}{\partial \mathbf{w} \partial \mathbf{w}^{\top}}(\mathbf{w})$ expensive)

% 3) Step-size selection: $\gamma >>$ might diverge. $\gamma <<$ slow convergence. Convergence to local minimum guaranteed only when $\gamma<\gamma_{\text {min }}$, $\gamma_{\min }$ depends on the problem.

% 4) Line-search methods: For some $\mathcal{L}$, set step-size automatically.

% 5) Feature normalization: 
% GD is sensitive to ill-conditioning $\Rightarrow$ Normalize your input features i.e. pre-condition the optimization problem. 

% \subsection*{Non-Convex Optimization}

% - Real-world problems are not convex!

% - All we have learnt on algorithm design and performance of convex algorithms still helps us in the nonconvex world.

% \subsection*{SGD Theory}
% - For convergence, $\gamma^{(t)}\rightarrow 0$ ``appropriately". Robbins-Monroe condition suggests to take $\gamma^{(t)}$ s.t.:

% $
% \sum_{t=1}^{\infty} \gamma^{(t)}=\infty, \quad \sum_{t=1}^{\infty}(\gamma^{(t)})^{2}<\infty
% $

% Example: $\gamma^{(t)}:=1 /(t+1)^{r}$ where $r \in(0.5,1)$.