
\section{Self-supervised Learning}
% - Problem: Labelled data for supervised learning is often limited and expensive to acquire.

% - Transfer learning: Fine-tune an existing model trained on a related task giving it a good internal representation of the input data. This can allow us to achieve high performance on the downstream task with significantly fewer labels. However, we still need supervision (labels) for the base task.

% - Self-Supervised Learning: Use the input itself to generate supervisory signals. The base task is an artificial pretext task that requires learning useful features and structure in the input data.

% In the pretext task we want to learn a model $f$ :

% $
% f: \mathbf{x}_{\text {in }} \mapsto \mathbf{x}_{\text {out }}
% $

% Where we use a transformation $g: \mathbf{x} \mapsto\left(\mathbf{x}_{\text {in }}, \mathbf{x}_{\text {out }}\right)$ to create an input and a target from an (unlabelled) datapoint $\mathbf{x} \in \mathcal{X}$. Note that $g$ often involves some randomness.


% \subsection*{Masked Language Modelling (MLM)}

% She [MASK] her coffee $\mapsto$ She drank her coffee

% - (backward) Data generation: corrupt the data

% - (forward) learning

\subsection*{BERT: (Bidirectional) Encoder Repr. from Transf.}
- encoder-only trained on masked language modelling.

- Encoder: whole sequence at once, every token can generally attend to every other token (both previous and later ones, hence bidirectional). Generates a fixed sized output, typically one token per input.



% - Training Inputs: BERT is trained on input sequences consisting of two sentences and special tokens, for example:

% \textit{
% [CLS] The cat is sleeping. [SEP] It's on the sofa. [SEP]
% }

% - [CLS] is a special classification token 

% - [SEP] is used to separate the two sentences. 

% - Around $15 \%$ of the standard tokens are selected to be replaced by [MASK]. 

% - Occasionally the selected tokens are replaced by a random word or not replaced to reduce distribution shift for downstream tasks that do not have [MASK]. 

% - Positional and segment encodings are added to the token embeddings.

\subsection*{BERT Training Objective:}
Predict original token. Softmax cross-entropy loss over all tks.

[CLS] token: order (binary classification).


% - Downstream tasks: Classifying sentences or pairs, Word level classification, Extracting a relevant passage

% \subsection*{Next Token Prediction}
% -Language Modelling Task

% - Predict the next token given previous tokens, e.g.: \textit{She drank her $\mapsto$ coffee}

% - Similar to Masked Language Modelling but is causal, we can only see previous words, not later ones. This makes it better suited for generating arbitrary length responses.

\subsection*{GPT: Generative Pre-trained Transformers}
- for Next Token Prediction: decoder-only transformer architecture
% Most generative Large Language Models (LLMs) like GPT-4 and LLAMA are members of this family. They can be fine tuned for a variety of purposes, famously as chatbots / assistants (ChatGPT, Bard).

% - Decoder: Each token can only attend to prior tokens (operates causally). 
% Used autoregressively during inference, each generated output token is fed back into the model as an input to generate the following output token.

% - Training Inputs: The training is simply done on token sequences with minimal changes e.g.:
% \textit{
% [EOS] The cat is sleeping. It's on the sofa. [EOS]
% }

% - [EOS] signifies the end of text and is used to separate unrelated chunks of text (but this can vary between implementations).

- Training Procedure: teacher forcing, 
% loss:classification loss over the possible vocabulary (softmax cross-entropy). Total loss for a sequence is the mean loss over all tokens in the sequence.
%  where we don't use the model outputs as subsequent inputs (like in inference) but rather use the actual correct sequence. $\rightarrow$ allows training on all tokens in a sequence to happen in parallel rather than sequentially. We need to mask the attention to prevent tokens from attending to future tokens, preserving the causality needed for inference.

% - loss for every token in the sequence corresponding to the prediction of the next token. This is a standard classification loss over the possible vocabulary (softmax cross-entropy). The total loss for a sequence is the mean loss over all tokens in the sequence.

% - Downstream Tasks: Decoders are very general sequenceto-sequence models and can be adapted to most natural language tasks. GPTs have been fine tuned for various applications. The base models are also often capable of performing various tasks without further tuning.

- In-Context Learning: prompting ChatGPT (prompt engineering)
% new tasks with just a few examples of successful completion provided as inputs, without any model updates. The model uses patterns it has learned during its initial training to infer how to handle these new tasks. e.g. translation.

% - Prompt engineering
% : search for an effective input (prompt) to achieve desired output behavior.

\subsection*{Joint Embedding Methods (Images): enc. invariant transfo.}
% Learn an encoder invariant to certain transformations on the data, e.g., to rotation rather than learning a model $f$ :


% $\mathbf{x}_{\text {in }} \mapsto \mathbf{x}_{\text {out }}$, learn $f: \mathcal{X} \rightarrow \mathbb{R}^{d}$ s.t.
% $
% f\left(\mathbf{x}_{1}\right) \approx f\left(\mathbf{x}_{2}\right)
% $
% where $g: \mathbf{x} \mapsto\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)$ is a transformation that creates two views from the same input datapoint $\mathbf{x} \in \mathcal{X}$.

% - $g$ is usually a random function that applies multiple data augmentations, each with certain probability.

% - Problem: constant $f$ is a trivial encoder that is invariant!

% - Solved by contrastive learning or non-contrastive methods with regularization, e.g.:

\begin{itemize}
  \item BYOL: use two encoders $f_{1}$, $f_{2}$, $f_{2}$ is exp moving average of $f_{1}$
\end{itemize}

% $
% f_{1}\left(\mathbf{x}_{1}\right) \approx f_{2}\left(\mathbf{x}_{2}\right)
% $

\begin{itemize}
  \item VicReg: force non-0 var. to avoid collapse but min the cov.
\end{itemize}



\subsection*{Contrastive learning}
% - Push away representations of unrelated views from different data pts!


- Given a positive pair $\mathbf{x}, \mathbf{x}^{+}$from the datapoint $\mathbf{x}$, and a negative view $\mathbf{x}^{-}$from a different datapoint $\mathbf{x}^{\prime} \neq \mathbf{x}$:
$\mathrm{s}\left(f(\mathbf{x}), f\left(\mathbf{x}^{+}\right)\right)>\mathrm{s}\left(f(\mathbf{x}), f\left(\mathbf{x}^{-}\right)\right)$,
s func quantifies the similarity of two embeddings.

\subsection*{SimCLR: contrastive learning}
% - A contrastive learning framework with optimized choices.

% \begin{enumerate}
%   \item classification-like objective with $N$ negative samples:
% \end{enumerate}
1. clf-like obj, $N$ neg samples
$
L(\mathbf{x})=-\mathbb{E}\left[\log \frac{\exp \left(\mathrm{s}\left(f(\mathbf{x}), f\left(\mathbf{x}^{+}\right)\right)\right)}{\sum_{i=1}^{N} \exp \left(\mathrm{s}\left(f(\mathbf{x}), f\left(\mathbf{x}_{i}^{-}\right)\right)\right)}\right]
$

max score between $\mathbf{x}$ and $\mathbf{x}^{+}$, min score between $\mathbf{x}$ and all $\mathbf{x}_{i}^{-}$.

\begin{enumerate}
  \setcounter{enumi}{1}
  \item use projector to map the encoder output to the similarity space:
  $
  f(\mathbf{x})=\underbrace{f_{2}}_{\text {projector }} \circ \underbrace{f_{1}}_{\text {encoder }}(\mathbf{x})
  $
  use only $f_{1}$ for downstream tasks.
\end{enumerate}


\begin{enumerate}
  \setcounter{enumi}{2}
  \item cosine similarity :
  $
  \mathrm{S}\left(\mathbf{e}_{1}, \mathbf{e}_{2}\right)=\frac{\left\langle\mathbf{e}_{1}, \mathbf{e}_{2}\right\rangle}{\left\|\mathbf{e}_{1}\right\|_{2}\left\|\mathbf{e}_{2}\right\|_{2}} / \tau
  $
\end{enumerate}


% \begin{enumerate}
%   \setcounter{enumi}{3}
%   \item How to $x \rightarrow x^+$? generate views with data augmentation:
% \end{enumerate}


% e.g. Rotate $\left\{90^{\circ}, 180^{\circ}, 270^{\circ}\right\}$, Crop, resize, Cutout, flip, Gaussian noise or blur, Color distort (drop or jitter), Sobel filtering

% \begin{itemize}
%   \item stronger data augmentations than those used in supervised learning,
%   \item random crop creates global to local $(B \rightarrow A)$ views or adjacent view $(D \rightarrow C)$ prediction tasks.
% \end{itemize}



\subsection*{CLIP: captioned images to learn a joint multimodal embedding space}
% - Use captioned images to learn a joint multimodal embedding space.

- Max cos sim. of caption and image embeds, min unrelated pairs.

- Few-shot learning: learn to clf new classes with only a few labelled examples. (CLIP able to zero shot)
% CLIP is able to zero-shot classify with the following text input: "A photo of [class]".

