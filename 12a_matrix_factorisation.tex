\section*{Matrix Factorization}

Given items (movies) $d=1, 2, . . . , D$ and users $n= 1, 2, . . . , N$, we define X to be the $D \times N$ matrix containing all rating entries. That is, $x_{dn}$ is the rating of n-th user for d-th item.
Note that most ratings $x_{dn}$ are missing, and our task is to predict them accurately.

\subsection*{Algorithm}

$X \approx WZ^T$, $W \in \mathbb{R}^{D \times K}, \ Z \in \mathbb{R}^{N \times K}$ tall matrices $K << N, D$

$\operatorname*{min}_{W,Z}~{\mathcal{L}}(W,Z):=\frac{1}{2}\sum_{(d,n)\in\mathbb{Q}} [x_{dn}-(WZ^T)_{dn}]^2$

- We hope to ``explain" each rating $x_{dn}$ by a numerical representation of the corresponding item and user - in fact by the inner product of an item feature vector with the user feature vector.



- The set $\Omega\subseteq\left[D\right]\times\left[N\right]$ collects the indices of the observed ratings of the input matrix $X$.

- This cost is not jointly convex w.r.t. W and Z, nor identifiable as $(w^*, z^*) \Leftrightarrow (\beta w^*, \beta^{-1} z^*)$

\subsection*{Choosing K}

- $\uparrow K \Rightarrow$ overfitting ($\Leftrightarrow \ \downarrow K \Rightarrow$ underfitting). 
For $K >> N,D \Rightarrow (W^*, Z^{*^T}) = (X, I) = (I, X)$

\subsection*{Regularization}

$\frac{1}{2}\sum_{(d,n)\in\Omega}[x_{d n}-(W Z^{T})_{d n}]^{2}+\frac{\lambda_{w}}{2}||{W}||_{\mathrm{Frob}}^{2}+\frac{\lambda_{z}}{2}||{Z}||_{\mathrm{Frob}}^{2}$
$\lambda_{w},\lambda_{z}\, \in \mathbb{R} > 0$


\subsection*{Stochastic Gradient Descent}

$\mathcal{L} = \frac{1}{|\Omega|} \sum_{(d,n)\in\Omega}\underbrace{{\frac{1}{2}}[x_{d n}-(\mathbf{W}\mathbf{Z}^{\textsf{T}})_{d n}]^{2}}_{f_{d,n}}$

For one fixed element (d, n) of the sum, we derive the gradient entry (d', k) for W:

${\frac{\partial}{\partial w_{d^{\prime},k}}}f_{d,n}(W, Z) \in \mathbb{R}^{D\times K}=\\\left\{\begin{array}{l l}{-\left[x_{d n}-({\bf W Z}^{T})_{d n}\right]z_{n,k}\;\;\;\mathrm{if}\;d^{\prime}=d} \\ {0 \mathrm{~otherwise}}\end{array}\right.$

${\frac{\partial}{\partial z_{n^{\prime},k}}}f_{d,n}(W, Z)\in \mathbb{R}^{N\times K}=\\\left\{\begin{array}{l l}{-\left[x_{d n}-({\bf W Z}^{T})_{d n}\right]w_{d,k}\;\;\;\mathrm{if}\;n^{\prime}=n} \\ {0 \mathrm{~otherwise}}\end{array}\right.$

- cost: $\Theta(K)$ which is cheap!

\subsection*{Alternating Least Squares}

- No missing entries:

${\textstyle\frac{1}{2}}\sum_{d=1}^{D}\sum_{n=1}^{N}\left[x_{d n}-\left(W\mathbf{Z}^{\mathsf{T}}\right)_{d n}\right]^{2}$
$\\=\frac{1}{2}\|\mathbf{X}-\mathbf{W}\mathbf{Z}^{\mathsf{T}}\|_{{Frob}}^{2}$

- We first minimize w.r.t. Z for fixed W and then minimize W given Z (closed form solutions):

$Z^{\mathsf{T}}:=(\mathsf{W}^{\mathsf{T}}\mathsf{W}+\lambda_{z}\mathsf{I}_{K})^{-1}\mathsf{W}^{\mathsf{T}}\mathsf{X}$

$\mathbf{W}^{\mathsf{T}}:=(\mathbf{Z}^{\mathsf{T}}\mathbf{Z}+\lambda_{w}\mathbf{I}_{K})^{-1}\mathbf{Z}^{\mathsf{T}}\mathbf{X}^{\mathsf{T}}$

- Cost: need to invert a $K \times K$ matrix

- With missing entries:
Can you derive the ALS updates for the more general setting, when only the ratings $(d, n) \in \Omega$ contribute to the cost, i.e.
$\frac{1}{2}\sum_{(d,n)\in\Omega}\big[x_{d n}-\big(\mathsf{W Z}^{\mathsf{T}}\big)_{d n}\big]^{2}$

Compute the gradient with respect to each group of variables, and set to zero.

% \subsection*{Update for $Z$ with fixed $W$:}
% The ALS update for $Z^{\mathsf{T}}$ is given by:
% \[
% Z^{\mathsf{T}} := \left(W^{\mathsf{T}}W + \lambda_{z}I_{K}\right)^{-1}W^{\mathsf{T}}X
% \]
% For each $(n, k)$ where $(d', n) \in \Omega$, the update for the $k$-th component of $Z^{\mathsf{T}}$ is:
% \[
% \left(Z^{\mathsf{T}}\right)_{n,k} := \left(\sum_{d':(d',n)\in\Omega} w_{d',k'}w_{d',k} + \lambda_{z}\delta_{k,k'}\right)^{-1} \sum_{d':(d',n)\in\Omega} w_{d',k'}x_{d',n}
% \]

% \subsection*{Update for $W$ with fixed $Z$:}
% The ALS update for $W^{\mathsf{T}}$ is given by:
% \[
% W^{\mathsf{T}} := \left(Z^{\mathsf{T}}Z + \lambda_{w}I_{K}\right)^{-1}Z^{\mathsf{T}}X^{\mathsf{T}}
% \]
% For each $(d, k)$ where $(d, n') \in \Omega$, the update for the $k$-th component of $W^{\mathsf{T}}$ is:
% \[
% \left(W^{\mathsf{T}}\right)_{d,k} := \left(\sum_{n':(d,n')\in\Omega} z_{n',k'}z_{n',k} + \lambda_{w}\delta_{k,k'}\right)^{-1} \sum_{n':(d,n')\in\Omega} z_{n',k'}x_{d,n'}
% \]