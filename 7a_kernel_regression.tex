\section{Kernel Regression}
$
\mathcal{W}_{*\text{ridge}}=\frac{1}{N}(\underbrace{\frac{1}{N} \mathbf{X}^{\top} \mathbf{X}+\lambda I_{d}}_{d \times d})^{-1} \mathbf{X}^{\top} \mathbf{y}
=\frac{1}{N} \mathbf{X}^{\top}(\underbrace{\frac{1}{N} \mathbf{X X}^{\top}+\lambda I_{N}}_{N \times N})^{-1} \mathbf{y}
$

- \underline{Proof}: Let $P \in \mathbb{R}^{m \times n}$ and $Q \in \mathbb{R}^{n \times m}$,
$
P\left(Q P+I_{n}\right)=P Q P+P=\left(P Q+I_{m}\right) P
$,
% Assuming that both $Q P+I_{n}$ and $P Q+I_{m}$ are invertible,
$
\left(P Q+I_{m}\right)^{-1} P=P\left(Q P+I_{n}\right)^{-1}
$,
$P=\mathbf{X}^{\top}$, $Q=\frac{1}{\lambda N} \mathbf{X}$

\subsection*{Usefulness of the alternative form}

1) Original $\rightarrow \mathcal{O}\left(d^{3}+N d^{2}\right)$,
Alternative $\rightarrow \mathcal{O}\left(N^{3}+d N^{2}\right)$

% $\Rightarrow$ Depending on $d, N$ one formulation may be more efficient than the other

2) Structural difference:
$
w_{*}=\mathbf{X}^{\top} \alpha_{*} \text { where } \alpha_{*}=\frac{1}{N}\left(\frac{1}{N} \mathbf{X} \mathbf{X}^{\top}+\lambda I_{N}\right)^{-1} \mathbf{y}
$
$\Rightarrow w_{*} \in \operatorname{span}\left\{x_{1}, \cdots, x_{N}\right\}$

% These two insights are fundamental to understanding the kernel trick

\subsection*{Representer Theorem}
- \underline{Claim}: For any loss function $\ell$, there exists $\alpha_{*} \in \mathbb{R}^{N}$ s.t.
$
w_{*}:=\mathbf{X}^{\top} \alpha_{*} \in \arg \min _{w} \frac{1}{N} \sum_{n=1}^{N} \ell\left(x_{n}^{\top} w, y_{n}\right)+\frac{\lambda}{2}\|w\|^{2}
$

- \underline{Meaning}: $\exists w_* \in \operatorname{span}\left\{x_{1}, \cdots, x_{N}\right\}$

- more general than LS, Kernel SVM, Kernel LS, and Kernel PCA

\subsection*{\underline{Proof} of the representer theorem}
- Let $w_{*}=\min _{w} \frac{1}{N} \sum_{n=1}^{N} \ell\left(x_{n}^{\top} w, y_{n}\right)+\frac{\lambda}{2}\|w\|^{2}=\sum_{n=1}^{N} \alpha_{n} x_{n}+u$ where $u^{\top} x_{n}=0$ for all $n$

- Let's define $w=w_{*}-u$,
$\left\|w_{*}\right\|^{2}=\|w\|^{2}+\|u\|^{2}\Rightarrow \|w\|^{2} \leq\left\|w_{*}\right\|^{2}$

$\forall n, w^{\top}x_{n}=\left(w_{*}-u\right)^{\top} x_{n}=w_{*}^{\top} x_{n}\Rightarrow\ell\left(x_{n}^{\top} w, y_{n}\right)=\ell\left(x_{n}^{\top} w_{*}, y_{n}\right)$

Therefore
$
\frac{1}{N} \sum_{n=1}^{N} \ell\left(x_{n}^{\top} w, y_{n}\right)+\frac{\lambda}{2}\|w\|^{2} \leq \frac{1}{N} \sum_{n=1}^{N} \ell\left(x_{n}^{\top} w_{*}, y_{n}\right)+\frac{\lambda}{2}\left\|w_{*}\right\|^{2}
$, and $w$ is an optimal solution for this problem.

\subsection*{Kernelized ridge regression}
% - Classic formulation in $w$ :
% $
% w_{*}=\arg \min _{w} \frac{1}{2 N}\|\mathbf{y}-\mathbf{X} w\|^{2}+\frac{\lambda}{2}\|w\|^{2}
% $

% - Alternative formulation in $\alpha$ :

$
\alpha_{*}=\arg \min _{\alpha} \frac{1}{2} \alpha^{\top}\left(\frac{1}{N} \mathbf{X} \mathbf{X}^{\top}+\lambda I_{N}\right) \alpha-\frac{1}{N} \alpha^{\top} \mathbf{y}
$

% - \underline{Claim}: Two formulations are equivalent

% - \underline{Proof}: Set the gradient to 0 , to obtain $\alpha_{*}$, and $w_{*}=\mathbf{X}^{\top} \alpha_{*}$

% - The dual formulation only uses $\mathbf{X}$ through $\mathbf{K}=\mathbf{X X}^{\top}$


% \subsection*{Kernel matrix}
% $
% \mathbf{K}=\mathbf{X} \mathbf{X}^{\top}=\left(\begin{array}{cccc}
% x_{1}^{\top} x_{1} & x_{1}^{\top} x_{2} & \cdots & x_{1}^{\top} x_{N} \\
% x_{2}^{\top} x_{1} & x_{2}^{\top} x_{2} & \cdots & x_{2}^{\top} x_{N} \\
% \vdots & \vdots & \ddots & \vdots \\
% x_{N}^{\top} x_{1} & x_{N}^{\top} x_{2} & \cdots & x_{N}^{\top} x_{N}
% \end{array}\right)=\left(x_{i}^{\top} x_{j}\right)_{i, j} \in \mathbb{R}^{N \times N}
% $

% - Depends on the dimensions only through the scalar product.

% - If we have a scalar product independent of the dimensions it can be fast.


\subsection*{Kernel matrix K with feature spaces}
- When a feature map $\phi: \mathbb{R}^{d} \rightarrow \mathbb{R}^{\tilde{d}}$ is used,
$
\left(x_{n}\right)_{n=1}^{N} \hookrightarrow\left(\phi\left(x_{n}\right)\right)_{n=1}^{N}
$,
$
\mathbf{K}=\boldsymbol{\Phi} \boldsymbol{\Phi}^{\top} \in \mathbb{R}^{N \times N}
$

- Problem: when $d \lll \tilde{d}$ computing $\phi(x)^{\top} \phi\left(x^{\prime}\right)$ costs $\mathcal{D}(\tilde{d})$

\subsection*{Kernel trick: Kernel function $\kappa(x,x^{\prime})=\phi(x)^\top\phi(x^{\prime})$}

Same: Compute $\kappa\left(x, x^{\prime}\right)$ or map features to $\phi(x)$ and compute $\phi(x)^{\top} \phi\left(x^{\prime}\right)$ $\Rightarrow$ compute linear classifiers in high-dimensional space without performing computations in it

\subsection*{Predicting with kernels: only using the kernel function}
% - Problem: The prediction is $y=\phi(x)^{\top} w_{*}$ but computing $\phi(x)$ can be expensive

% - Question: How can we make predictions using only the kernel function, without the need to compute $\phi(x)$ ?

$\phi(x)^{\top} w_{*}=\phi(x)^{\top} \phi(\mathbf{X})^{\top} \alpha_{*}=\sum_{n=1}^{N} \kappa\left(x, x_{n}\right) \alpha_{*_{n}}$ 

$y={\phi(x)^\top w_*}_{\text{Linear pred in feature space}}={f_{W_*}(x)}_{\text{Non linear pred in $\mathcal{X}$ space}}$

\subsection*{1. Linear Kernel: $\kappa\left(x, x^{\prime}\right)=x^{\top} x^{\prime}$, $\phi(x)=x$}

% $\kappa\left(x, x^{\prime}\right)=x^{\top} x^{\prime}$, $\phi(x)=x$

\subsection*{2. Quadratic kernel: $\kappa\left(x, x^{\prime}\right)=\left(x x^{\prime}\right)^{2}$, $x, x^{\prime} \in \mathbb{R}$, $\phi(x)=x^{2}$}

% $\kappa\left(x, x^{\prime}\right)=\left(x x^{\prime}\right)^{2}$, $x, x^{\prime} \in \mathbb{R}$ $\Rightarrow$, $\phi(x)=x^{2}$

\subsection*{3. Polynomial kernel}
Let $x, x^{\prime} \in \mathbb{R}^{3}$,
$
\kappa\left(x, x^{\prime}\right)=\left(x_{1} x_{1}^{\prime}+x_{2} x_{2}^{\prime}+x_{3} x_{3}^{\prime}\right)^{2}
$

$
\phi(x)=\left[x_{1}^{2}, x_{2}^{2}, x_{3}^{2}, \sqrt{2} x_{1} x_{2}, \sqrt{2} x_{1} x_{3}, \sqrt{2} x_{2} x_{3}\right] \in \mathbb{R}^{6}
$

% - \underline{Proof}:
% $
% \kappa\left(x, x^{\prime}\right) =\phi(x)^{\top} \phi\left(x^{\prime}\right) 
% =\left(x_{1} x_{1}^{\prime}+x_{2} x_{2}^{\prime}+x_{3} x_{3}^{\prime}\right)^{2} 
% =\left(x_{1} x_{1}^{\prime}\right)^{2}+\left(x_{2} x_{2}^{\prime}\right)^{2}+\left(x_{3} x_{3}^{\prime}\right)^{2}+2 x_{1} x_{2} x_{1}^{\prime} x_{2}^{\prime}+2 x_{1} x_{3} x_{1}^{\prime} x_{3}^{\prime}+2 x_{2} x_{3} x_{2}^{\prime} x_{3}^{\prime} 
% =\left(x_{1}^{2}, x_{2}^{2}, x_{3}^{2}, \sqrt{2} x_{1} x_{2}, \sqrt{2} x_{1} x_{3}, \sqrt{2} x_{2} x_{3}\right)^{\top} \times
% \\ \left(x_{1}^{\prime 2}, x_{2}^{\prime 2}, x_{3}^{\prime 2}, \sqrt{2} x_{1}^{\prime} x_{2}^{\prime}, \sqrt{2} x_{1}^{\prime} x_{3}^{\prime}, \sqrt{2} x_{2}^{\prime} x_{3}^{\prime}\right)
% $

\subsection*{4. Radial basis function (RBF) kernel}
- Let $x, x^{\prime} \in \mathbb{R}^{d}$
$
\kappa\left(x, x^{\prime}\right)=e^{-\left(x-x^{\prime}\right)^{\top}\left(x-x^{\prime}\right)}
$

- e.g. 
$
\phi(x)=e^{-x^{2}}\left(\cdots, \frac{2^{k / 2} x^{k}}{\sqrt{k !}} \cdots\right)
$

% - \underline{Proof}: $\quad \kappa\left(x, x^{\prime}\right)=e^{-x^{2}-x^{2}+2 x x^{\prime}}
% =e^{-x^{2}} e^{-x^{\prime 2}} \sum_{k=0}^{\infty} \frac{2^{k} x^{k} x^{\prime k}}{k !}$
% (Taylor: $e^{x}=\sum_{k=0}^{\infty}\frac{x^{k}}{k!}$),
% $
% \phi(x)=e^{-x^{2}}\left(\cdots, \frac{2^{k / 2} x^{k}}{\sqrt{k !}} \cdots\right) \Longrightarrow \phi(x)^{\top} \phi\left(x^{\prime}\right)=\kappa\left(x, x^{\prime}\right)
% $,
% (cannot be represented as an inner product in a finite-dimensional space)

\subsection*{Building new kernels from existing kernels}
% - Let $\kappa_{1}, \kappa_{2}$ be two kernel functions and $\phi_{1}, \phi_{2}$ the corresponding feature maps

\underline{{Claim} 1}: Positive linear combinations of kernel are kernels:

$
\kappa\left(x, x^{\prime}\right)=\alpha \kappa_{1}\left(x, x^{\prime}\right)+\beta \kappa_{2}\left(x, x^{\prime}\right) \text { for } \alpha, \beta \geq 0
$

\underline{Proof 1}: 
$
% \kappa\left(x, x^{\prime}\right)=\alpha \kappa_{1}\left(x, x^{\prime}\right)+\beta \kappa_{2}\left(x, x^{\prime}\right)
% =\alpha \phi_{1}(x)^{\top} \phi_{1}\left(x^{\prime}\right)+\beta \phi_{2}(x)^{\top} \phi_{2}\left(x^{\prime}\right)
% =\phi(x)^{\top} \phi\left(x^{\prime}\right), 
\phi(x)=\left(\begin{array}{c}
\sqrt{\alpha} \phi_{1}(x) \\
\sqrt{\beta} \phi_{2}(x)
\end{array}\right) \in \mathbb{R}^{d_{1}+d_{2}}
$

\underline{{Claim} 2}: Products of kernels are kernels:
% $
% \kappa\left(x, x^{\prime}\right)=\kappa_{1}\left(x, x^{\prime}\right) \kappa_{2}\left(x, x^{\prime}\right)
% $

\underline{Proof 2}:
% $\kappa\left(x, x^{\prime}\right)
% =\phi_{1}(x)^{\top} \phi_{1}\left(x^{\prime}\right) \phi_{2}(x)^{\top} \phi_{2}\left(x^{\prime}\right)
% $
Let 
$\phi(x)^{\top} = (\phi_{1}(x)_{1}\phi_{2}(x)_{1}, \cdots, \phi_{1}(x)_{1}\phi_{2}(x)_{d_{2}}, \\\cdots,
\phi_{1}(x)_{d_{1}}\phi_{2}(x)_{1}, \cdots, \phi_{1}(x)_{d_{1}}\phi_{2}(x)_{d_{2}}) \in \mathbb{R}^{d_{1} d_{2}}$
then

$
\phi(x)^{\top} \phi\left(x^{\prime}\right) =\sum_{i, j}\left(\phi_{1}(x)\right)_{i}\left(\phi_{2}(x)\right)_{j}\left(\phi_{1}\left(x^{\prime}\right)\right)_{i}\left(\phi_{2}\left(x^{\prime}\right)\right)_{j} 
=\sum_{i}\left(\phi_{1}(x)\right)_{i}\left(\phi_{1}\left(x^{\prime}\right)\right)_{i} \sum_{j}\left(\phi_{2}(x)\right)_{j}\left(\phi_{2}\left(x^{\prime}\right)\right)_{j} 
=\phi_{1}(x)^{\top} \phi_{1}\left(x^{\prime}\right) \phi_{2}(x)^{\top} \phi_{2}\left(x^{\prime}\right)=\kappa\left(x, x^{\prime}\right)
$

\subsection*{Mercer's condition}
- Given $\kappa$, $\exists\phi$ s.t.
$
\kappa\left(x, x^{\prime}\right)=\phi(x)^{\top} \phi\left(x^{\prime}\right)
$
iff:

1) The kernel function is symmetric:
$
\forall x, x^{\prime}, \kappa\left(x, x^{\prime}\right)=\kappa\left(x^{\prime}, x\right)
$

2) The kernel matrix is psd for all possible input sets:
$
\forall N \geq 0, \forall\left(x_{n}\right)_{n=1}^{N}, \quad \mathbf{K}=\left(\kappa\left(x_{i}, x_{j}\right)\right)_{i, j=1}^{N} \geqslant 0
$

\subsection*{\underline{Proof} of Mercer theorem}
$\bullet$ If $\kappa$ represents an inner product then it is symmetric and the kernel matrix is psd:
$
v^{\top} \mathrm{K} v=\sum_{i, j} v_{i} v_{j} \phi\left(x_{i}\right)^{\top} \phi\left(x_{j}\right)=\left\|\sum_{i} v_{i} \phi\left(x_{i}\right)\right\|^{2}
$

$\bullet$ Define $\phi(x)=\kappa(\cdot, x)$. Define a vector space of functions by considering all linear combinations $\left\{\sum_{i} \alpha_{i} \kappa\left(\cdot, x_{i}\right)\right\}$. Define an inner product on this vector space by

$
\left\langle\sum_{i} \alpha_{i} \kappa\left(\cdot, x_{i}\right), \sum_{j} \beta_{j} \kappa\left(\cdot, x_{j}^{\prime}\right)\right\rangle=\sum_{i, j} \alpha_{i} \beta_{j} \kappa\left(x_{i}, x_{j}^{\prime}\right)
$

This is a valid inner product (symmetric, bilinear and positive definite, with equality holding only if $\phi(x)$ is the zero function)

Consequently
$
\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle=\left\langle\kappa(\cdot, x), \kappa\left(\cdot, x^{\prime}\right)\right\rangle=\kappa\left(x, x^{\prime}\right)
$
